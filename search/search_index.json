{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Roerich Roerich is a python library for online and offline change point detection for time series analysis, signal processing, and segmentation. It was named after the painter Nicholas Roerich, known as the Master of the Mountains. Read more at: https://www.roerich.org. Fragment of \"Himalayas\", 1933 Currently, the library contains official implementations of change point detection algorithms based on direct density ratio estimation from the following articles: Mikhail Hushchyn and Andrey Ustyuzhanin. \u201cGeneralization of Change-Point Detection in Time Series Data Based on Direct Density Ratio Estimation.\u201d J. Comput. Sci. 53 (2021): 101385. [journal] [arxiv] Mikhail Hushchyn, Kenenbek Arzymatov and Denis Derkach. \u201cOnline Neural Networks for Change-Point Detection.\u201d ArXiv abs/2010.01388 (2020). [arxiv] Dependencies and install pip install roerich or git clone https://github.com/HSE-LAMBDA/roerich.git cd roerich pip install -e . Basic usage (See more examples in the documentation .) The following code snippet generates a noisy synthetic data, performs change point detection, and displays the results. If you use own dataset, make sure that it has a shape (seq_len, n_dims) . import roerich from roerich.change_point import ChangePointDetectionClassifier # generate time series X, cps_true = roerich.generate_dataset(period=200, N_tot=2000) # detection # base_classifier = 'logreg', 'qda', 'dt', 'rf', 'mlp', 'knn', 'nb' # metric = 'klsym', 'pesym', 'jsd', 'mmd', 'fd' cpd = ChangePointDetectionClassifier(base_classifier='mlp', metric='klsym', window_size=100) score, cps_pred = cpd.predict(X) # visualization roerich.display(X, cps_true, score, cps_pred) Support Home: https://github.com/HSE-LAMBDA/roerich Documentation: https://hse-lambda.github.io/roerich For any usage questions, suggestions and bugs use the issue page , please. Related libraries Thanks to all our contributors","title":"Home"},{"location":"#welcome-to-roerich","text":"Roerich is a python library for online and offline change point detection for time series analysis, signal processing, and segmentation. It was named after the painter Nicholas Roerich, known as the Master of the Mountains. Read more at: https://www.roerich.org. Fragment of \"Himalayas\", 1933 Currently, the library contains official implementations of change point detection algorithms based on direct density ratio estimation from the following articles: Mikhail Hushchyn and Andrey Ustyuzhanin. \u201cGeneralization of Change-Point Detection in Time Series Data Based on Direct Density Ratio Estimation.\u201d J. Comput. Sci. 53 (2021): 101385. [journal] [arxiv] Mikhail Hushchyn, Kenenbek Arzymatov and Denis Derkach. \u201cOnline Neural Networks for Change-Point Detection.\u201d ArXiv abs/2010.01388 (2020). [arxiv]","title":"Welcome to Roerich"},{"location":"#dependencies-and-install","text":"pip install roerich or git clone https://github.com/HSE-LAMBDA/roerich.git cd roerich pip install -e .","title":"Dependencies and install"},{"location":"#basic-usage","text":"(See more examples in the documentation .) The following code snippet generates a noisy synthetic data, performs change point detection, and displays the results. If you use own dataset, make sure that it has a shape (seq_len, n_dims) . import roerich from roerich.change_point import ChangePointDetectionClassifier # generate time series X, cps_true = roerich.generate_dataset(period=200, N_tot=2000) # detection # base_classifier = 'logreg', 'qda', 'dt', 'rf', 'mlp', 'knn', 'nb' # metric = 'klsym', 'pesym', 'jsd', 'mmd', 'fd' cpd = ChangePointDetectionClassifier(base_classifier='mlp', metric='klsym', window_size=100) score, cps_pred = cpd.predict(X) # visualization roerich.display(X, cps_true, score, cps_pred)","title":"Basic usage"},{"location":"#support","text":"Home: https://github.com/HSE-LAMBDA/roerich Documentation: https://hse-lambda.github.io/roerich For any usage questions, suggestions and bugs use the issue page , please.","title":"Support"},{"location":"#related-libraries","text":"","title":"Related libraries"},{"location":"#thanks-to-all-our-contributors","text":"","title":"Thanks to all our contributors"},{"location":"about/","text":"Change point Formal definition Abrupt change of time series behaviour is called a change-point. Consider a d- dimensional signal with a change point at a moment \\nu : x_1, x_2, \\cdots, x_{\\nu-1}, x_{\\nu}, x_{\\nu+1}, x_{\\nu+2}, \\cdots In the most general case, the observations are arbitrary dependent and nonidentically distributed. Change point is a moment of time, when distribution of time series observations changes. Thus, it can be described as changing of conditional pre-change densities p_0(x_i|x_1, \\cdots, x_{i-1}) for i \\le \\nu to conditional post-change densities p_1(x_i|x_1, \\cdots, x_{i-1}) for i > \\nu . In a typical setup this is an unsupervised task with unknown signal distributions. Figure 1 shows an example with several change points and results of their detection. Figure 1. Example of change points. Applications There is a range of various applications of change point detection [1]: quality control of production process, structural health monitoring of wind turbines, and aircraft, detecting multiple sensor faults, detecting road traffic incidents or changes in highway traffic condition, chemical process control, physiological data analysis, surveillance of daily disease counts, nanoscale analysis of soft biomaterials, biosurveillance, radio-astronomy and interferometry, spectrum sensing in cognitive radio systems, leak detection in water channels, environmental monitoring, handling climate changes, navigation systems monitoring, human motion analysis, video scene analysis, sequential steganography, biometric identification, onset detection in music signals, detecting changes in large payment card datasets, distributed systems monitoring, detection of intrusion, viruses, and other denial of service (DoS) attacks, segmentation of signals and images, tracking the preferences of users in recommendation systems, seismic data processing, analysis of financial data and others. [1] Alexander Tartakovsky, Igor Nikiforov, and Michele Basseville. 2014. Sequential Analysis: Hypothesis Testing and Changepoint Detection (1st. ed.). Chapman & Hall/CRC.","title":"Change point"},{"location":"about/#change-point","text":"","title":"Change point"},{"location":"about/#formal-definition","text":"Abrupt change of time series behaviour is called a change-point. Consider a d- dimensional signal with a change point at a moment \\nu : x_1, x_2, \\cdots, x_{\\nu-1}, x_{\\nu}, x_{\\nu+1}, x_{\\nu+2}, \\cdots In the most general case, the observations are arbitrary dependent and nonidentically distributed. Change point is a moment of time, when distribution of time series observations changes. Thus, it can be described as changing of conditional pre-change densities p_0(x_i|x_1, \\cdots, x_{i-1}) for i \\le \\nu to conditional post-change densities p_1(x_i|x_1, \\cdots, x_{i-1}) for i > \\nu . In a typical setup this is an unsupervised task with unknown signal distributions. Figure 1 shows an example with several change points and results of their detection. Figure 1. Example of change points.","title":"Formal definition"},{"location":"about/#applications","text":"There is a range of various applications of change point detection [1]: quality control of production process, structural health monitoring of wind turbines, and aircraft, detecting multiple sensor faults, detecting road traffic incidents or changes in highway traffic condition, chemical process control, physiological data analysis, surveillance of daily disease counts, nanoscale analysis of soft biomaterials, biosurveillance, radio-astronomy and interferometry, spectrum sensing in cognitive radio systems, leak detection in water channels, environmental monitoring, handling climate changes, navigation systems monitoring, human motion analysis, video scene analysis, sequential steganography, biometric identification, onset detection in music signals, detecting changes in large payment card datasets, distributed systems monitoring, detection of intrusion, viruses, and other denial of service (DoS) attacks, segmentation of signals and images, tracking the preferences of users in recommendation systems, seismic data processing, analysis of financial data and others. [1] Alexander Tartakovsky, Igor Nikiforov, and Michele Basseville. 2014. Sequential Analysis: Hypothesis Testing and Changepoint Detection (1st. ed.). Chapman & Hall/CRC.","title":"Applications"},{"location":"cpdclassifier/","text":"ChangePointDetectionClassifier Description ChangePointDetectionClassifier [1] is an offline change point detection algorithm based on binary classifiers. It scans a signal with a two consecutive windows with some width. The method uses a binary classifier for direct estimation of the probability density ratio for observations inside these windows. Then, it calculates the symmetrical Kullback-Leibler (KL_sym) divergence based on these ratios and considers it as a change point detection score for the windows pair. [1] Mikhail Hushchyn and Andrey Ustyuzhanin. \u201cGeneralization of Change-Point Detection in Time Series Data Based on Direct Density Ratio Estimation.\u201d J. Comput. Sci. 53 (2021): 101385. [journal] [arxiv] Usage import roerich from roerich.change_point import ChangePointDetectionClassifier # generate time series X, cps_true = roerich.generate_dataset(period=200, N_tot=2000) # change points detection # base_classifier = 'logreg', 'qda', 'dt', 'rf', 'mlp', 'knn', 'nb' # metric = 'klsym', 'pesym', 'jsd', 'mmd', 'fd' cpd = ChangePointDetectionClassifier(base_classifier='mlp', metric='klsym', periods=1, window_size=100, step=1, n_runs=1) score, cps_pred = cpd.predict(X) # visualization roerich.display(X, cps_true, score, cps_pred) Usage with custom classifier from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis # sklearn-like binary classifier clf = QuadraticDiscriminantAnalysis() # change points detection cpd = ChangePointDetectionClassifier(base_classifier=clf, metric='klsym', periods=1, window_size=100, step=1, n_runs=1) score, cps_pred = cpd.predict(X) # visualization roerich.display(X, cps_true, score, cps_pred)","title":"ChangePointDetectionClassifier"},{"location":"cpdclassifier/#changepointdetectionclassifier","text":"","title":"ChangePointDetectionClassifier"},{"location":"cpdclassifier/#description","text":"ChangePointDetectionClassifier [1] is an offline change point detection algorithm based on binary classifiers. It scans a signal with a two consecutive windows with some width. The method uses a binary classifier for direct estimation of the probability density ratio for observations inside these windows. Then, it calculates the symmetrical Kullback-Leibler (KL_sym) divergence based on these ratios and considers it as a change point detection score for the windows pair. [1] Mikhail Hushchyn and Andrey Ustyuzhanin. \u201cGeneralization of Change-Point Detection in Time Series Data Based on Direct Density Ratio Estimation.\u201d J. Comput. Sci. 53 (2021): 101385. [journal] [arxiv]","title":"Description"},{"location":"cpdclassifier/#usage","text":"import roerich from roerich.change_point import ChangePointDetectionClassifier # generate time series X, cps_true = roerich.generate_dataset(period=200, N_tot=2000) # change points detection # base_classifier = 'logreg', 'qda', 'dt', 'rf', 'mlp', 'knn', 'nb' # metric = 'klsym', 'pesym', 'jsd', 'mmd', 'fd' cpd = ChangePointDetectionClassifier(base_classifier='mlp', metric='klsym', periods=1, window_size=100, step=1, n_runs=1) score, cps_pred = cpd.predict(X) # visualization roerich.display(X, cps_true, score, cps_pred)","title":"Usage"},{"location":"cpdclassifier/#usage-with-custom-classifier","text":"from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis # sklearn-like binary classifier clf = QuadraticDiscriminantAnalysis() # change points detection cpd = ChangePointDetectionClassifier(base_classifier=clf, metric='klsym', periods=1, window_size=100, step=1, n_runs=1) score, cps_pred = cpd.predict(X) # visualization roerich.display(X, cps_true, score, cps_pred)","title":"Usage with custom classifier"},{"location":"cpdclassifier_cv/","text":"ChangePointDetectionClassifierCV Description ChangePointDetectionClassifierCV is a version of ChangePointDetectionClassifier [1] method with K-Fold cross-validation. [1] Mikhail Hushchyn and Andrey Ustyuzhanin. \u201cGeneralization of Change-Point Detection in Time Series Data Based on Direct Density Ratio Estimation.\u201d J. Comput. Sci. 53 (2021): 101385. [journal] [arxiv] Usage import roerich from roerich.change_point import ChangePointDetectionClassifierCV # generate time series X, cps_true = roerich.generate_dataset(period=200, N_tot=2000) # change points detection # base_classifier = 'logreg', 'qda', 'dt', 'rf', 'mlp', 'knn', 'nb' # metric = 'klsym', 'pesym', 'jsd', 'mmd', 'fd' cpd = ChangePointDetectionClassifierCV(base_classifier='nb', metric='klsym', periods=1, window_size=100, step=1, n_splits=5) score, cps_pred = cpd.predict(X) # visualization roerich.display(X, cps_true, score, cps_pred) Usage with custom classifier from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis # sklearn-like binary classifier clf = QuadraticDiscriminantAnalysis() # change points detection cpd = ChangePointDetectionClassifierCV(base_classifier=clf, metric='klsym', periods=1, window_size=100, step=1, n_splits=5) score, cps_pred = cpd.predict(X) # visualization roerich.display(X, cps_true, score, cps_pred)","title":"ChangePointDetectionClassifierCV"},{"location":"cpdclassifier_cv/#changepointdetectionclassifiercv","text":"","title":"ChangePointDetectionClassifierCV"},{"location":"cpdclassifier_cv/#description","text":"ChangePointDetectionClassifierCV is a version of ChangePointDetectionClassifier [1] method with K-Fold cross-validation. [1] Mikhail Hushchyn and Andrey Ustyuzhanin. \u201cGeneralization of Change-Point Detection in Time Series Data Based on Direct Density Ratio Estimation.\u201d J. Comput. Sci. 53 (2021): 101385. [journal] [arxiv]","title":"Description"},{"location":"cpdclassifier_cv/#usage","text":"import roerich from roerich.change_point import ChangePointDetectionClassifierCV # generate time series X, cps_true = roerich.generate_dataset(period=200, N_tot=2000) # change points detection # base_classifier = 'logreg', 'qda', 'dt', 'rf', 'mlp', 'knn', 'nb' # metric = 'klsym', 'pesym', 'jsd', 'mmd', 'fd' cpd = ChangePointDetectionClassifierCV(base_classifier='nb', metric='klsym', periods=1, window_size=100, step=1, n_splits=5) score, cps_pred = cpd.predict(X) # visualization roerich.display(X, cps_true, score, cps_pred)","title":"Usage"},{"location":"cpdclassifier_cv/#usage-with-custom-classifier","text":"from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis # sklearn-like binary classifier clf = QuadraticDiscriminantAnalysis() # change points detection cpd = ChangePointDetectionClassifierCV(base_classifier=clf, metric='klsym', periods=1, window_size=100, step=1, n_splits=5) score, cps_pred = cpd.predict(X) # visualization roerich.display(X, cps_true, score, cps_pred)","title":"Usage with custom classifier"},{"location":"cpdrulsif/","text":"ChangePointDetectionRuLSIF Description ChangePointDetectionRuLSIF [1] is an offline change point detection algorithm based on RuLSIF [2, 3] regressors. It scans a signal with a two consecutive windows with some width. The method uses a RuLSIF regression for direct estimation of the probability density ratio for observations inside these windows. Then, it calculates the Pearson (PE) divergence based on these ratios and considers it as a change point detection score for the windows pair. [1] Mikhail Hushchyn and Andrey Ustyuzhanin. \u201cGeneralization of Change-Point Detection in Time Series Data Based on Direct Density Ratio Estimation.\u201d J. Comput. Sci. 53 (2021): 101385. [journal] [arxiv] [2] Makoto Yamada, Taiji Suzuki, Takafumi Kanamori, Hirotaka Hachiya, Masashi Sugiyama; Relative Density-Ratio Estimation for Robust Distribution Comparison. Neural Comput 2013; 25 (5): 1324\u20131370. [DOI] [3] Song Liu, Makoto Yamada, Nigel Collier, Masashi Sugiyama, Change-point detection in time-series data by relative density-ratio estimation, Neural Networks, V. 43, 2013, pp. 72-83. [DOI] Usage import roerich from roerich.change_point import ChangePointDetectionRuLSIF # generate time series X, cps_true = roerich.generate_dataset(period=200, N_tot=2000) # change points detection cpd = ChangePointDetectionRuLSIF(periods=1, window_size=100, step=5, n_runs=1) score, cps_pred = cpd.predict(X) # visualization roerich.display(X, cps_true, score, cps_pred)","title":"ChangePointDetectionRuLSIF"},{"location":"cpdrulsif/#changepointdetectionrulsif","text":"","title":"ChangePointDetectionRuLSIF"},{"location":"cpdrulsif/#description","text":"ChangePointDetectionRuLSIF [1] is an offline change point detection algorithm based on RuLSIF [2, 3] regressors. It scans a signal with a two consecutive windows with some width. The method uses a RuLSIF regression for direct estimation of the probability density ratio for observations inside these windows. Then, it calculates the Pearson (PE) divergence based on these ratios and considers it as a change point detection score for the windows pair. [1] Mikhail Hushchyn and Andrey Ustyuzhanin. \u201cGeneralization of Change-Point Detection in Time Series Data Based on Direct Density Ratio Estimation.\u201d J. Comput. Sci. 53 (2021): 101385. [journal] [arxiv] [2] Makoto Yamada, Taiji Suzuki, Takafumi Kanamori, Hirotaka Hachiya, Masashi Sugiyama; Relative Density-Ratio Estimation for Robust Distribution Comparison. Neural Comput 2013; 25 (5): 1324\u20131370. [DOI] [3] Song Liu, Makoto Yamada, Nigel Collier, Masashi Sugiyama, Change-point detection in time-series data by relative density-ratio estimation, Neural Networks, V. 43, 2013, pp. 72-83. [DOI]","title":"Description"},{"location":"cpdrulsif/#usage","text":"import roerich from roerich.change_point import ChangePointDetectionRuLSIF # generate time series X, cps_true = roerich.generate_dataset(period=200, N_tot=2000) # change points detection cpd = ChangePointDetectionRuLSIF(periods=1, window_size=100, step=5, n_runs=1) score, cps_pred = cpd.predict(X) # visualization roerich.display(X, cps_true, score, cps_pred)","title":"Usage"},{"location":"energy/","text":"EnergyDistanceCalculator Description EnergyDistanceCalculator is an offline change point detection algorithm based on energy distance [1]. It scans a signal with a two consecutive windows with some width. The method calculates the energy distance for observations inside these windows, and considers it as a change point detection score for the windows pair. [1] Szekely, Gabor. (2003). E-Statistics: The energy of statistical samples. Usage import roerich from roerich.change_point import EnergyDistanceCalculator # generate time series X, cps_true = roerich.generate_dataset(period=200, N_tot=2000) # change points detection cpd = EnergyDistanceCalculator(window_size=100, step=1) score, cps_pred = cpd.predict(X) # visualization roerich.display(X, cps_true, score, cps_pred)","title":"EnergyDistanceCalculator"},{"location":"energy/#energydistancecalculator","text":"","title":"EnergyDistanceCalculator"},{"location":"energy/#description","text":"EnergyDistanceCalculator is an offline change point detection algorithm based on energy distance [1]. It scans a signal with a two consecutive windows with some width. The method calculates the energy distance for observations inside these windows, and considers it as a change point detection score for the windows pair. [1] Szekely, Gabor. (2003). E-Statistics: The energy of statistical samples.","title":"Description"},{"location":"energy/#usage","text":"import roerich from roerich.change_point import EnergyDistanceCalculator # generate time series X, cps_true = roerich.generate_dataset(period=200, N_tot=2000) # change points detection cpd = EnergyDistanceCalculator(window_size=100, step=1) score, cps_pred = cpd.predict(X) # visualization roerich.display(X, cps_true, score, cps_pred)","title":"Usage"},{"location":"gbdtrulsif/","text":"GBDTRuLSIFRegressor Description GBDTRuLSIFRegressor [1] is an algorithm for the direct density ratio estimation for two samples. It is a Gradient Boosting over Decision Trees model with the RuLSIF [2] loss function. The algorithm takes two samples and learns the ratio of their probability densities without estimation of individual distributions. The density ratios help to calculate of different dissimilarity scores between the samples, that are used in change-point detection and other applications. [1] Mikhail Hushchyn and Andrey Ustyuzhanin. \u201cGeneralization of Change-Point Detection in Time Series Data Based on Direct Density Ratio Estimation.\u201d J. Comput. Sci. 53 (2021): 101385. [journal] [arxiv] [2] Makoto Yamada, Taiji Suzuki, Takafumi Kanamori, Hirotaka Hachiya, Masashi Sugiyama; Relative Density-Ratio Estimation for Robust Distribution Comparison. Neural Comput 2013; 25 (5): 1324\u20131370. [DOI] Usage from roerich.density_ratio import GBDTRuLSIFRegressor from scipy.stats import norm, uniform import matplotlib.pyplot as plt import numpy as np N = 10000 # generate samples p0 = uniform(-5, 10) p1 = norm(0, 1) X = np.concatenate((p0.rvs((N, 1)), p1.rvs((N, 1)))) y = np.array([0]*N + [1]*N) # true density ratio true_ratio = p1.pdf(X) / p0.pdf(X) # direct density ratio estimation reg = GBDTRuLSIFRegressor(n_estimators=100, learning_rate=0.2, max_depth=2, alpha=0) reg.fit(X, y) pred_ratio = reg.predict_proba_ratio(X) # visualization plt.scatter(X, pred_ratio, label='pred') plt.scatter(X, true_ratio, label='true') plt.legend() plt.show()","title":"GBDTRuLSIFRegressor"},{"location":"gbdtrulsif/#gbdtrulsifregressor","text":"","title":"GBDTRuLSIFRegressor"},{"location":"gbdtrulsif/#description","text":"GBDTRuLSIFRegressor [1] is an algorithm for the direct density ratio estimation for two samples. It is a Gradient Boosting over Decision Trees model with the RuLSIF [2] loss function. The algorithm takes two samples and learns the ratio of their probability densities without estimation of individual distributions. The density ratios help to calculate of different dissimilarity scores between the samples, that are used in change-point detection and other applications. [1] Mikhail Hushchyn and Andrey Ustyuzhanin. \u201cGeneralization of Change-Point Detection in Time Series Data Based on Direct Density Ratio Estimation.\u201d J. Comput. Sci. 53 (2021): 101385. [journal] [arxiv] [2] Makoto Yamada, Taiji Suzuki, Takafumi Kanamori, Hirotaka Hachiya, Masashi Sugiyama; Relative Density-Ratio Estimation for Robust Distribution Comparison. Neural Comput 2013; 25 (5): 1324\u20131370. [DOI]","title":"Description"},{"location":"gbdtrulsif/#usage","text":"from roerich.density_ratio import GBDTRuLSIFRegressor from scipy.stats import norm, uniform import matplotlib.pyplot as plt import numpy as np N = 10000 # generate samples p0 = uniform(-5, 10) p1 = norm(0, 1) X = np.concatenate((p0.rvs((N, 1)), p1.rvs((N, 1)))) y = np.array([0]*N + [1]*N) # true density ratio true_ratio = p1.pdf(X) / p0.pdf(X) # direct density ratio estimation reg = GBDTRuLSIFRegressor(n_estimators=100, learning_rate=0.2, max_depth=2, alpha=0) reg.fit(X, y) pred_ratio = reg.predict_proba_ratio(X) # visualization plt.scatter(X, pred_ratio, label='pred') plt.scatter(X, true_ratio, label='true') plt.legend() plt.show()","title":"Usage"},{"location":"matrix/","text":"MatrixImportance Description MatrixImportance calculates a matrix of importance for each pair of input features. It helps to understand impact of the features into the change point detection score. Usage import numpy as np from roerich.explanation import MatrixImportance from roerich.change_point import EnergyDistanceCalculator # generate time series n = 400 X1 = np.random.normal([0, 0, 0], 1, size=(n, 3)) X2 = np.random.normal([0, 1, 2], 1, size=(n, 3)) X = np.concatenate((X1, X2), axis=0) cps_true = [n] # base detection algo cp = EnergyDistanceCalculator(window_size=100) # importance mi = MatrixImportance(cp) # change point on all input features score, _ = mi.predict(X) # change point on each pair of features mat_u = mi.predict_union(X) # change point with excluding pairs of features mat_e = mi.predict_exclude(X) # visualization roerich.display(X, cps_true, mat_u[:, 1, 2], None)","title":"MatrixImportance"},{"location":"matrix/#matriximportance","text":"","title":"MatrixImportance"},{"location":"matrix/#description","text":"MatrixImportance calculates a matrix of importance for each pair of input features. It helps to understand impact of the features into the change point detection score.","title":"Description"},{"location":"matrix/#usage","text":"import numpy as np from roerich.explanation import MatrixImportance from roerich.change_point import EnergyDistanceCalculator # generate time series n = 400 X1 = np.random.normal([0, 0, 0], 1, size=(n, 3)) X2 = np.random.normal([0, 1, 2], 1, size=(n, 3)) X = np.concatenate((X1, X2), axis=0) cps_true = [n] # base detection algo cp = EnergyDistanceCalculator(window_size=100) # importance mi = MatrixImportance(cp) # change point on all input features score, _ = mi.predict(X) # change point on each pair of features mat_u = mi.predict_union(X) # change point with excluding pairs of features mat_e = mi.predict_exclude(X) # visualization roerich.display(X, cps_true, mat_u[:, 1, 2], None)","title":"Usage"},{"location":"nnclassifier/","text":"NNClassifier Description NNClassifier [1] is an algorithm for the direct density ratio estimation for two samples. It is a Neural Network model for binary classification task. The algorithm takes two samples and learns the ratio of their probability densities without estimation of individual distributions. The classifier learns to separate the samples into two classes. According to [1], the density ratio w(x) can be estimated using predictions f(x) of the classifier: w(x) = \\frac{f(x)}{1 - f(x)} The density ratios help to calculate of different dissimilarity scores between the samples, that are used in change-point detection and other applications. [1] Mikhail Hushchyn and Andrey Ustyuzhanin. \u201cGeneralization of Change-Point Detection in Time Series Data Based on Direct Density Ratio Estimation.\u201d J. Comput. Sci. 53 (2021): 101385. [journal] [arxiv] Usage from roerich.density_ratio import NNClassifier from scipy.stats import norm, uniform import matplotlib.pyplot as plt import numpy as np N = 1000 # generate samples p0 = uniform(-5, 10) p1 = norm(0, 1) X = np.concatenate((p0.rvs((N, 1)), p1.rvs((N, 1)))) y = np.array([0]*N + [1]*N) # true density ratio true_ratio = p1.pdf(X) / p0.pdf(X) # direct density ratio estimation reg = NNClassifier(n_hidden=10, n_epochs=100, batch_size=64, lr=0.01, l2=0.001) reg.fit(X, y) pred_ratio = reg.predict_proba_ratio(X) # visualization plt.scatter(X, pred_ratio, label='pred') plt.scatter(X, true_ratio, label='true') plt.legend() plt.show()","title":"NNClassifier"},{"location":"nnclassifier/#nnclassifier","text":"","title":"NNClassifier"},{"location":"nnclassifier/#description","text":"NNClassifier [1] is an algorithm for the direct density ratio estimation for two samples. It is a Neural Network model for binary classification task. The algorithm takes two samples and learns the ratio of their probability densities without estimation of individual distributions. The classifier learns to separate the samples into two classes. According to [1], the density ratio w(x) can be estimated using predictions f(x) of the classifier: w(x) = \\frac{f(x)}{1 - f(x)} The density ratios help to calculate of different dissimilarity scores between the samples, that are used in change-point detection and other applications. [1] Mikhail Hushchyn and Andrey Ustyuzhanin. \u201cGeneralization of Change-Point Detection in Time Series Data Based on Direct Density Ratio Estimation.\u201d J. Comput. Sci. 53 (2021): 101385. [journal] [arxiv]","title":"Description"},{"location":"nnclassifier/#usage","text":"from roerich.density_ratio import NNClassifier from scipy.stats import norm, uniform import matplotlib.pyplot as plt import numpy as np N = 1000 # generate samples p0 = uniform(-5, 10) p1 = norm(0, 1) X = np.concatenate((p0.rvs((N, 1)), p1.rvs((N, 1)))) y = np.array([0]*N + [1]*N) # true density ratio true_ratio = p1.pdf(X) / p0.pdf(X) # direct density ratio estimation reg = NNClassifier(n_hidden=10, n_epochs=100, batch_size=64, lr=0.01, l2=0.001) reg.fit(X, y) pred_ratio = reg.predict_proba_ratio(X) # visualization plt.scatter(X, pred_ratio, label='pred') plt.scatter(X, true_ratio, label='true') plt.legend() plt.show()","title":"Usage"},{"location":"nnrulsif/","text":"NNRuLSIFRegressor Description NNRuLSIFRegressor [1] is an algorithm for the direct density ratio estimation for two samples. It is a Neural Network model with the RuLSIF [2] loss function. The algorithm takes two samples and learns the ratio of their probability densities without estimation of individual distributions. The density ratios help to calculate of different dissimilarity scores between the samples, that are used in change-point detection and other applications. [1] Mikhail Hushchyn and Andrey Ustyuzhanin. \u201cGeneralization of Change-Point Detection in Time Series Data Based on Direct Density Ratio Estimation.\u201d J. Comput. Sci. 53 (2021): 101385. [journal] [arxiv] [2] Makoto Yamada, Taiji Suzuki, Takafumi Kanamori, Hirotaka Hachiya, Masashi Sugiyama; Relative Density-Ratio Estimation for Robust Distribution Comparison. Neural Comput 2013; 25 (5): 1324\u20131370. [DOI] Usage from roerich.density_ratio import NNRuLSIFRegressor from scipy.stats import norm, uniform import matplotlib.pyplot as plt import numpy as np N = 1000 # generate samples p0 = uniform(-5, 10) p1 = norm(0, 1) X = np.concatenate((p0.rvs((N, 1)), p1.rvs((N, 1)))) y = np.array([0]*N + [1]*N) # true density ratio true_ratio = p1.pdf(X) / p0.pdf(X) # direct density ratio estimation reg = NNRuLSIFRegressor(n_hidden=10, n_epochs=100, batch_size=64, lr=0.01, l2=0.001, alpha=0.) reg.fit(X, y) pred_ratio = reg.predict_proba_ratio(X) # visualization plt.scatter(X, pred_ratio, label='pred') plt.scatter(X, true_ratio, label='true') plt.legend() plt.show()","title":"NNRuLSIFRegressor"},{"location":"nnrulsif/#nnrulsifregressor","text":"","title":"NNRuLSIFRegressor"},{"location":"nnrulsif/#description","text":"NNRuLSIFRegressor [1] is an algorithm for the direct density ratio estimation for two samples. It is a Neural Network model with the RuLSIF [2] loss function. The algorithm takes two samples and learns the ratio of their probability densities without estimation of individual distributions. The density ratios help to calculate of different dissimilarity scores between the samples, that are used in change-point detection and other applications. [1] Mikhail Hushchyn and Andrey Ustyuzhanin. \u201cGeneralization of Change-Point Detection in Time Series Data Based on Direct Density Ratio Estimation.\u201d J. Comput. Sci. 53 (2021): 101385. [journal] [arxiv] [2] Makoto Yamada, Taiji Suzuki, Takafumi Kanamori, Hirotaka Hachiya, Masashi Sugiyama; Relative Density-Ratio Estimation for Robust Distribution Comparison. Neural Comput 2013; 25 (5): 1324\u20131370. [DOI]","title":"Description"},{"location":"nnrulsif/#usage","text":"from roerich.density_ratio import NNRuLSIFRegressor from scipy.stats import norm, uniform import matplotlib.pyplot as plt import numpy as np N = 1000 # generate samples p0 = uniform(-5, 10) p1 = norm(0, 1) X = np.concatenate((p0.rvs((N, 1)), p1.rvs((N, 1)))) y = np.array([0]*N + [1]*N) # true density ratio true_ratio = p1.pdf(X) / p0.pdf(X) # direct density ratio estimation reg = NNRuLSIFRegressor(n_hidden=10, n_epochs=100, batch_size=64, lr=0.01, l2=0.001, alpha=0.) reg.fit(X, y) pred_ratio = reg.predict_proba_ratio(X) # visualization plt.scatter(X, pred_ratio, label='pred') plt.scatter(X, true_ratio, label='true') plt.legend() plt.show()","title":"Usage"},{"location":"onnc/","text":"OnlineNNClassifier Description OnlineNNClassifier [1] is an online change point detection algorithm based on a binary classifier. It scans a signal with a two windows with small width (1, 5, 10) and a lag (50, 100) between them. The method uses a neural network classifier for direct estimation of the probability density ratio for observations inside these windows. We train the NN in online manner: update its weights with new observations of the signal. Then, it calculates the symmetrical Kullback-Leibler (KL_sym) divergence based on these ratios and considers it as a change point detection score for the windows pair. [1] Mikhail Hushchyn, Kenenbek Arzymatov and Denis Derkach. \u201cOnline Neural Networks for Change-Point Detection.\u201d ArXiv abs/2010.01388 (2020). [arxiv] Usage import roerich from roerich.change_point import OnlineNNClassifier # generate time series X, cps_true = roerich.generate_dataset(period=200, N_tot=2000) # change points detection cpd = OnlineNNClassifier(periods=1, window_size=1, lag_size=100, step=1, n_epochs=1, lr=0.01, lam=0.0001, optimizer=\"Adam\") score, cps_pred = cpd.predict(X) # visualization roerich.display(X, cps_true, score, cps_pred) Usage with custom network import torch.nn as nn # custom network class MyNN(nn.Module): def __init__(self, n_inputs=1): super(MyNN, self).__init__() self.net = nn.Sequential(nn.Linear(n_inputs, 100), nn.ReLU(), nn.Linear(100, 100), nn.ReLU(), nn.Linear(100, 1), nn.Sigmoid()) def forward(self, x): return self.net(x) # change points detection cpd = OnlineNNClassifier(net=MyNN, periods=1, window_size=1, lag_size=100, step=1, n_epochs=1, lr=0.001, lam=0.0001, optimizer=\"Adam\") score, cps_pred = cpd.predict(X) # visualization roerich.display(X, cps_true, score, cps_pred)","title":"OnlineNNClassifier"},{"location":"onnc/#onlinennclassifier","text":"","title":"OnlineNNClassifier"},{"location":"onnc/#description","text":"OnlineNNClassifier [1] is an online change point detection algorithm based on a binary classifier. It scans a signal with a two windows with small width (1, 5, 10) and a lag (50, 100) between them. The method uses a neural network classifier for direct estimation of the probability density ratio for observations inside these windows. We train the NN in online manner: update its weights with new observations of the signal. Then, it calculates the symmetrical Kullback-Leibler (KL_sym) divergence based on these ratios and considers it as a change point detection score for the windows pair. [1] Mikhail Hushchyn, Kenenbek Arzymatov and Denis Derkach. \u201cOnline Neural Networks for Change-Point Detection.\u201d ArXiv abs/2010.01388 (2020). [arxiv]","title":"Description"},{"location":"onnc/#usage","text":"import roerich from roerich.change_point import OnlineNNClassifier # generate time series X, cps_true = roerich.generate_dataset(period=200, N_tot=2000) # change points detection cpd = OnlineNNClassifier(periods=1, window_size=1, lag_size=100, step=1, n_epochs=1, lr=0.01, lam=0.0001, optimizer=\"Adam\") score, cps_pred = cpd.predict(X) # visualization roerich.display(X, cps_true, score, cps_pred)","title":"Usage"},{"location":"onnc/#usage-with-custom-network","text":"import torch.nn as nn # custom network class MyNN(nn.Module): def __init__(self, n_inputs=1): super(MyNN, self).__init__() self.net = nn.Sequential(nn.Linear(n_inputs, 100), nn.ReLU(), nn.Linear(100, 100), nn.ReLU(), nn.Linear(100, 1), nn.Sigmoid()) def forward(self, x): return self.net(x) # change points detection cpd = OnlineNNClassifier(net=MyNN, periods=1, window_size=1, lag_size=100, step=1, n_epochs=1, lr=0.001, lam=0.0001, optimizer=\"Adam\") score, cps_pred = cpd.predict(X) # visualization roerich.display(X, cps_true, score, cps_pred)","title":"Usage with custom network"},{"location":"onnr/","text":"OnlineNNRuLSIF Description OnlineNNRuLSIF [1] is an online change point detection algorithm based on a RuLSIF [2, 3] regressor. It scans a signal with a two windows with small width (1, 5, 10) and a lag (50, 100) between them. The method uses a neural network RuLSIF regressor for direct estimation of the probability density ratio for observations inside these windows. We train the NN in online manner: update its weights with new observations of the signal. Then, it calculates the Pearson (PE) divergence based on these ratios and considers it as a change point detection score for the windows pair. [1] Mikhail Hushchyn, Kenenbek Arzymatov and Denis Derkach. \u201cOnline Neural Networks for Change-Point Detection.\u201d ArXiv abs/2010.01388 (2020). [arxiv] [2] Makoto Yamada, Taiji Suzuki, Takafumi Kanamori, Hirotaka Hachiya, Masashi Sugiyama; Relative Density-Ratio Estimation for Robust Distribution Comparison. Neural Comput 2013; 25 (5): 1324\u20131370. [DOI] [3] Song Liu, Makoto Yamada, Nigel Collier, Masashi Sugiyama, Change-point detection in time-series data by relative density-ratio estimation, Neural Networks, V. 43, 2013, pp. 72-83. [DOI] Usage import roerich from roerich.change_point import OnlineNNRuLSIF # generate time series X, cps_true = roerich.generate_dataset(period=200, N_tot=2000) # change points detection cpd = OnlineNNRuLSIF(alpha=0.1, window_size=1, lag_size=100, step=1, n_epochs=1, lr=0.01, lam=0.001, optimizer=\"Adam\") score, cps_pred = cpd.predict(X) # visualization roerich.display(X, cps_true, score, cps_pred) Usage with custom network import torch.nn as nn # custom network class MyNN(nn.Module): def __init__(self, n_inputs=1): super(MyNN, self).__init__() self.net = nn.Sequential(nn.Linear(n_inputs, 100), nn.ReLU(), nn.Linear(100, 100), nn.ReLU(), nn.Linear(100, 1)) def forward(self, x): return self.net(x) # change points detection cpd = OnlineNNRuLSIF(net=MyNN, alpha=0.1, window_size=1, lag_size=100, step=1, n_epochs=1, lr=0.001, lam=0.001, optimizer=\"Adam\") score, cps_pred = cpd.predict(X) # visualization roerich.display(X, cps_true, score, cps_pred)","title":"OnlineNNRuLSIF"},{"location":"onnr/#onlinennrulsif","text":"","title":"OnlineNNRuLSIF"},{"location":"onnr/#description","text":"OnlineNNRuLSIF [1] is an online change point detection algorithm based on a RuLSIF [2, 3] regressor. It scans a signal with a two windows with small width (1, 5, 10) and a lag (50, 100) between them. The method uses a neural network RuLSIF regressor for direct estimation of the probability density ratio for observations inside these windows. We train the NN in online manner: update its weights with new observations of the signal. Then, it calculates the Pearson (PE) divergence based on these ratios and considers it as a change point detection score for the windows pair. [1] Mikhail Hushchyn, Kenenbek Arzymatov and Denis Derkach. \u201cOnline Neural Networks for Change-Point Detection.\u201d ArXiv abs/2010.01388 (2020). [arxiv] [2] Makoto Yamada, Taiji Suzuki, Takafumi Kanamori, Hirotaka Hachiya, Masashi Sugiyama; Relative Density-Ratio Estimation for Robust Distribution Comparison. Neural Comput 2013; 25 (5): 1324\u20131370. [DOI] [3] Song Liu, Makoto Yamada, Nigel Collier, Masashi Sugiyama, Change-point detection in time-series data by relative density-ratio estimation, Neural Networks, V. 43, 2013, pp. 72-83. [DOI]","title":"Description"},{"location":"onnr/#usage","text":"import roerich from roerich.change_point import OnlineNNRuLSIF # generate time series X, cps_true = roerich.generate_dataset(period=200, N_tot=2000) # change points detection cpd = OnlineNNRuLSIF(alpha=0.1, window_size=1, lag_size=100, step=1, n_epochs=1, lr=0.01, lam=0.001, optimizer=\"Adam\") score, cps_pred = cpd.predict(X) # visualization roerich.display(X, cps_true, score, cps_pred)","title":"Usage"},{"location":"onnr/#usage-with-custom-network","text":"import torch.nn as nn # custom network class MyNN(nn.Module): def __init__(self, n_inputs=1): super(MyNN, self).__init__() self.net = nn.Sequential(nn.Linear(n_inputs, 100), nn.ReLU(), nn.Linear(100, 100), nn.ReLU(), nn.Linear(100, 1)) def forward(self, x): return self.net(x) # change points detection cpd = OnlineNNRuLSIF(net=MyNN, alpha=0.1, window_size=1, lag_size=100, step=1, n_epochs=1, lr=0.001, lam=0.001, optimizer=\"Adam\") score, cps_pred = cpd.predict(X) # visualization roerich.display(X, cps_true, score, cps_pred)","title":"Usage with custom network"},{"location":"pr/","text":"Precision and Recall Precision and Recall Consider a time series with n change-points at moments \\tau_{1} , \\tau_{2} , ..., \\tau_{n} . Suppose that an algorithm recognises m change points at moments \\hat{\\tau}_{1} , \\hat{\\tau}_{2} , ..., \\hat{\\tau}_{m} . Following [1], a set of correctly detected change-points is defined as True Positive (TP): \\text{TP} = \\{ \\tau_{i} | \\exists \\hat{\\tau}_{j}: |\\hat{\\tau}_{j} - \\tau_{i}| < M \\} where M is a margin size, maximum distance allowed between true and predicted change points. Then, Precision and Recall metrics are calculated as follows: \\text{Precision} = \\frac{|\\text{TP}|}{m} \\text{Recall} = \\frac{|\\text{TP}|}{n} [1] C. Truong, L. Oudre, N. Vayatis. Selective review of offline change point detection methods. Signal Processing, 167:107299, 2020. [journal] PR curve Suppose that we know detection scores s_{1} , s_{2} , ..., s_{m} for all m recognised change points. Let's calculate Precision and Recall metrics for different threshold values \\nu for the scores: \\text{TP}(\\nu) = \\{ \\tau_{i} | \\exists \\hat{\\tau}_{j}: |\\hat{\\tau}_{j} - \\tau_{i}| < M \\text{ and } (s_j \\ge \\nu) \\} m(\\nu) = \\sum_{j: s_j \\ge \\nu} 1 \\text{Precision}(\\nu) = \\frac{|\\text{TP}(\\nu)|}{m(\\nu)} \\text{Recall}(\\nu) = \\frac{|\\text{TP}(\\nu)|}{n} PR curve is the dependency of Precision (\\nu) form Recall (\\nu) . Usage from roerich.metrics import precision_recall_scores, precision_recall_curve, auc_score import matplotlib.pyplot as plt cps_true = [100, 200, 300, 400, 500] cps_pred = [105, 230, 310, 350, 405, 490] cps_score_pred = [1, 2, 3, 0.1, 5, 6] # precision and recall precision, recall = precision_recall_scores(cps_true, cps_pred, window=20) print('Precision: ', precision) print('Recall: ', recall) # PR curve and AUC thr, precision, recall = precision_recall_curve(cps_true, cps_pred, cps_score_pred, window=20) auc = auc_score(thr, precision, recall) print(\"PR AUC: \", auc) # visualization plt.plot(recall, precision) plt.show()","title":"Precision and Recall"},{"location":"pr/#precision-and-recall","text":"","title":"Precision and Recall"},{"location":"pr/#precision-and-recall_1","text":"Consider a time series with n change-points at moments \\tau_{1} , \\tau_{2} , ..., \\tau_{n} . Suppose that an algorithm recognises m change points at moments \\hat{\\tau}_{1} , \\hat{\\tau}_{2} , ..., \\hat{\\tau}_{m} . Following [1], a set of correctly detected change-points is defined as True Positive (TP): \\text{TP} = \\{ \\tau_{i} | \\exists \\hat{\\tau}_{j}: |\\hat{\\tau}_{j} - \\tau_{i}| < M \\} where M is a margin size, maximum distance allowed between true and predicted change points. Then, Precision and Recall metrics are calculated as follows: \\text{Precision} = \\frac{|\\text{TP}|}{m} \\text{Recall} = \\frac{|\\text{TP}|}{n} [1] C. Truong, L. Oudre, N. Vayatis. Selective review of offline change point detection methods. Signal Processing, 167:107299, 2020. [journal]","title":"Precision and Recall"},{"location":"pr/#pr-curve","text":"Suppose that we know detection scores s_{1} , s_{2} , ..., s_{m} for all m recognised change points. Let's calculate Precision and Recall metrics for different threshold values \\nu for the scores: \\text{TP}(\\nu) = \\{ \\tau_{i} | \\exists \\hat{\\tau}_{j}: |\\hat{\\tau}_{j} - \\tau_{i}| < M \\text{ and } (s_j \\ge \\nu) \\} m(\\nu) = \\sum_{j: s_j \\ge \\nu} 1 \\text{Precision}(\\nu) = \\frac{|\\text{TP}(\\nu)|}{m(\\nu)} \\text{Recall}(\\nu) = \\frac{|\\text{TP}(\\nu)|}{n} PR curve is the dependency of Precision (\\nu) form Recall (\\nu) .","title":"PR curve"},{"location":"pr/#usage","text":"from roerich.metrics import precision_recall_scores, precision_recall_curve, auc_score import matplotlib.pyplot as plt cps_true = [100, 200, 300, 400, 500] cps_pred = [105, 230, 310, 350, 405, 490] cps_score_pred = [1, 2, 3, 0.1, 5, 6] # precision and recall precision, recall = precision_recall_scores(cps_true, cps_pred, window=20) print('Precision: ', precision) print('Recall: ', recall) # PR curve and AUC thr, precision, recall = precision_recall_curve(cps_true, cps_pred, cps_score_pred, window=20) auc = auc_score(thr, precision, recall) print(\"PR AUC: \", auc) # visualization plt.plot(recall, precision) plt.show()","title":"Usage"},{"location":"window/","text":"SlidingWindows Description SlidingWindows is an offline change point detection algorithm based on discrepancy measures [1]. It scans a signal with a two consecutive windows with some width. The method calculates discrepancies between observations inside these windows, and considers it as a change point detection score for the windows pair. Implemented discrepancy measures: Energy distance ('energy') Frechet distance ('fd') Maximum Mean Discrepancy ('mmd') [1] C. Truong, L. Oudre, N. Vayatis. Selective review of offline change point detection methods. Signal Processing, 167:107299, 2020. Usage import roerich from roerich.change_point import SlidingWindows # generate time series X, cps_true = roerich.generate_dataset(period=200, N_tot=2000) # change points detection # metric = 'fd', 'mmd', 'energy' cpd = SlidingWindows(metric='mmd', window_size=100) score, cps_pred = cpd.predict(X) # visualization roerich.display(X, cps_true, score, cps_pred)","title":"SlidingWindows"},{"location":"window/#slidingwindows","text":"","title":"SlidingWindows"},{"location":"window/#description","text":"SlidingWindows is an offline change point detection algorithm based on discrepancy measures [1]. It scans a signal with a two consecutive windows with some width. The method calculates discrepancies between observations inside these windows, and considers it as a change point detection score for the windows pair. Implemented discrepancy measures: Energy distance ('energy') Frechet distance ('fd') Maximum Mean Discrepancy ('mmd') [1] C. Truong, L. Oudre, N. Vayatis. Selective review of offline change point detection methods. Signal Processing, 167:107299, 2020.","title":"Description"},{"location":"window/#usage","text":"import roerich from roerich.change_point import SlidingWindows # generate time series X, cps_true = roerich.generate_dataset(period=200, N_tot=2000) # change points detection # metric = 'fd', 'mmd', 'energy' cpd = SlidingWindows(metric='mmd', window_size=100) score, cps_pred = cpd.predict(X) # visualization roerich.display(X, cps_true, score, cps_pred)","title":"Usage"},{"location":"examples/benchmarking-Copy1/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Benchmarking Description This notebook compares the quality of algorithms in Roerich , Ruptures and klcpd libraries. We use \u201cWISDM Smartphone and Smartwatch Activity and Biometrics Dataset\u201d [1, 2], prepared by the Wireless Sensor Data Mining (WISDM) Lab in the Department of Computer and Information Science of Fordham University. The dataset includes data from the accelerometer and gyroscope sensors of a smartphone and smartwatch collected as 51 subjects performed 18 diverse activities of daily living. Each activity was performed for 3 minutes, so that each subject contributed 54 minutes of data. These activities include basic ambulation-related activities (e.g., walking, jogging, climbing stairs), hand-based activities of daily living (e.g., brushing teeth, folding clothes), and various eating activities (eating pasta, easting chips). Each subject had a smartwatch placed on his/her dominant hand and a smartphone in their pocket. The data collection was controlled by a custom-made app that ran on the smartphone and smartwatch. The sensor data was collected at a rate of 20 Hz (i.e., every 50ms) from the accelerometer and gyroscope on both the smartphone and smartwatch, yielding four total sensors. [1] Weiss, Gary. (2019). WISDM Smartphone and Smartwatch Activity and Biometrics Dataset . UCI Machine Learning Repository. [link] [2] Weiss, Gary & Yoneda, Kenichi & Hayajneh, Thaier. (2019). Smartphone and Smartwatch-Based Biometrics Using Activities of Daily Living. IEEE Access. PP. 1-1. [DOI] . Data Download The dataset is stored in UCI Machine Learning Repository and available for download via this link . #!wget -q https://archive.ics.uci.edu/ml/machine-learning-databases/00507/wisdm-dataset.zip #!unzip -o -q wisdm-dataset.zip In this example, we use data from a smartwatch accelerometer collected from one subject. The data file contains the following columns: id: uniquely identifies the subject. Rand: 1600 - 1650; activity: identifies a specific activity. Range: A-S (no \u201cN\u201d value); timestamp: Linux time; x: sensor value for x axis. May be positive or negative; y: same as x but for y axis; z: same as x but for z axis; Let\u2019s download the dataset and read a sample. #!pip install -q roerich #import of basic libraries import matplotlib.pyplot as plt import pandas as pd import numpy as np import roerich import torch # helping function for reading the data def remove_char(fname): with open(fname, \"r\") as f: flist = f.readlines() flist = [s.replace(';', '') for s in flist] with open(fname[:-4] + '_debug.txt', \"w\") as f: f.writelines(flist) # path to a file in the dataset data_path = \"wisdm-dataset/raw/watch/accel/data_1600_accel_watch.txt\" remove_char(data_path) # read the data cols = [\"id\", \"activity\", \"timestamp\", \"x\", \"y\", \"z\"] data = pd.read_csv(data_path[:-4] + '_debug.txt', names=cols) data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id activity timestamp x y z 0 1600 A 90426708196641 7.091625 -0.591667 8.195502 1 1600 A 90426757696641 4.972757 -0.158317 6.696732 2 1600 A 90426807196641 3.253720 -0.191835 6.107758 3 1600 A 90426856696641 2.801216 -0.155922 5.997625 4 1600 A 90426906196641 3.770868 -1.051354 7.731027 .colab-df-quickchart { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-quickchart:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-quickchart { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-quickchart:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } <script> async function quickchart(key) { const containerElement = document.querySelector('#' + key); const charts = await google.colab.kernel.invokeFunction( 'suggestCharts', [key], {}); } </script> <script> function displayQuickchartButton(domScope) { let quickchartButtonEl = domScope.querySelector('#df-c149b3fc-b2df-4368-9c0e-c3ff5db5a86b button.colab-df-quickchart'); quickchartButtonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; } displayQuickchartButton(document); </script> <style> .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } <script> const buttonEl = document.querySelector('#df-178cb2c5-f226-4743-9fa0-0221616266ff button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-178cb2c5-f226-4743-9fa0-0221616266ff'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } </script> Preprocessing We will use only sensor measurements and activity labels. So, let\u2019s define the input matrix X and the label vector y. We will take every 20'th observation in order to speed up the computation. #Taking only every 20th observation X = data[['x', 'y', 'z']].values[::20] y = data['activity'].values[::20] However, we do not need the activity labels by itself, but moments of the activity changes. We consider them as change points of the signal, and our goal is to detect them. # transform activity labels into change point positions def get_true_cpds(y): cps_true = [] for i in range(1, len(y)): if y[i] != y[i-1]: cps_true.append(i) return np.array(cps_true) # get true change point positions cps_true = get_true_cpds(y) Then, we preprocess the time series using StandardScaler . It is not mandatory in general case, but sometimes can be useful :). Besides this, we add some noise to our signal. This hack is a regularization technique that helps to reduce sensitivity of the change point detection methods. It decreases the number of change points within one activity. from sklearn.preprocessing import StandardScaler # scale the signal ss = StandardScaler().fit(X) X_ss = ss.transform(X) # add noise for regularization X_ss += np.random.normal(0, 0.1, X_ss.shape) roerich.display(X_ss, cps_true) Comparison of algorithms In this section we will compare algorithms from the Roerich library with a few from other libraries ( Ruptures and klcpd ) Roerich #Default parameters (taken from `Usage` tab in documentation) base_clf = 'mlp' base_reg = 'mlp' base_window_size = 50 base_lag_size = 50 base_periods = 1 base_n_runs = 1 base_step = 1 base_n_epochs = 10 base_n_splits = 5 base_alpha = 0.2 base_lr = 0.01 base_lam = 0.0001 base_optimizer = 'Adam' from roerich.change_point import ChangePointDetectionRuLSIF, ChangePointDetectionClassifier, \\ EnergyDistanceCalculator, ChangePointDetectionClassifierCV, \\ OnlineNNRuLSIF, OnlineNNClassifier, SlidingWindows algorithms = [(ChangePointDetectionClassifier(base_classifier=base_clf, metric='KL_sym', periods=base_periods, window_size=base_window_size, step=base_step, n_runs=base_n_runs), 'ChangePointDetectionClassifier'), (ChangePointDetectionClassifierCV(base_classifier=base_clf, metric='KL_sym', periods=base_periods, window_size=base_window_size, step=base_step, n_splits=base_n_splits), 'ChangePointDetectionClassifierCV'), (ChangePointDetectionRuLSIF(base_regressor=base_reg, metric='PE', periods=base_periods, window_size=base_window_size, step=base_step, n_runs=base_n_runs), 'ChangePointDetectionRuLSIF'), (OnlineNNRuLSIF(alpha=base_alpha, net='auto', scaler='auto', periods=base_periods, window_size=1, #algorithm's feature lag_size=base_lag_size, step=base_step, n_epochs=base_n_epochs, lr=base_lr, lam=base_lam, optimizer=base_optimizer), 'OnlineNNRuLSIF'), (OnlineNNClassifier(net='auto', scaler='auto', metric='KL_sym', periods=base_periods, window_size=1, #algorithm's feature lag_size=base_lag_size, step=base_step, n_epochs=base_n_epochs, lr=base_lr, lam=base_lam, optimizer=base_optimizer), 'OnlineNNClassifier'), (EnergyDistanceCalculator(window_size=base_window_size, step=base_step), 'EnergyDistanceCalculator'), (SlidingWindows(periods=base_periods, window_size=base_window_size, step=base_step, n_runs=base_n_runs), 'SlidingWindows')] We will write a function that takes an array of true change-points, predicted ones and the scores and returns an array of three metrics: precision, recall and AUC-PR. from roerich.metrics import precision_recall_scores, precision_recall_curve, auc_score def calculate_metrics(cps_true, cps_pred, score): precision, recall = precision_recall_scores(cps_true, cps_pred, window=base_window_size) results = [precision, recall] thr, precision, recall = precision_recall_curve(cps_true, cps_pred, score[cps_pred], window=base_window_size) results.append(auc_score(thr, precision, recall)) return results The results will be stored in a dataframe: the columns are different metrics and the rows are the results for different algorithms. df = pd.DataFrame(columns=['algorithm', 'precision', 'recall', 'AUC-PR']) df.set_index('algorithm', inplace=True) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } precision recall AUC-PR algorithm .colab-df-quickchart { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-quickchart:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-quickchart { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-quickchart:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } <script> async function quickchart(key) { const containerElement = document.querySelector('#' + key); const charts = await google.colab.kernel.invokeFunction( 'suggestCharts', [key], {}); } </script> <script> function displayQuickchartButton(domScope) { let quickchartButtonEl = domScope.querySelector('#df-d5b27179-5a0e-4c6c-9088-a6ac6a22b560 button.colab-df-quickchart'); quickchartButtonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; } displayQuickchartButton(document); </script> <style> .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } <script> const buttonEl = document.querySelector('#df-61cb2dd7-9a49-4e90-807e-15fd8f4d22a3 button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-61cb2dd7-9a49-4e90-807e-15fd8f4d22a3'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } </script> #Iterating through all the algorithms scores = [] #We store scores to plot them later for cpd, alg_name in algorithms: score, cps_pred = cpd.predict(X_ss) df.loc[alg_name] = calculate_metrics(cps_true, cps_pred, score) scores.append(score) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } precision recall AUC-PR algorithm ChangePointDetectionClassifier 0.850000 1.0 1.000000 ChangePointDetectionClassifierCV 0.772727 1.0 0.996636 ChangePointDetectionRuLSIF 0.739130 1.0 0.932255 OnlineNNRuLSIF 0.894737 1.0 0.985207 OnlineNNClassifier 0.894737 1.0 0.989269 EnergyDistanceCalculator 0.894737 1.0 1.000000 SlidingWindows 0.894737 1.0 0.993636 .colab-df-quickchart { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-quickchart:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-quickchart { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-quickchart:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } <script> async function quickchart(key) { const containerElement = document.querySelector('#' + key); const charts = await google.colab.kernel.invokeFunction( 'suggestCharts', [key], {}); } </script> <script> function displayQuickchartButton(domScope) { let quickchartButtonEl = domScope.querySelector('#df-8865d658-9955-4523-b629-8a9113faa452 button.colab-df-quickchart'); quickchartButtonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; } displayQuickchartButton(document); </script> <style> .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } <script> const buttonEl = document.querySelector('#df-9675e229-885a-48a5-b26d-d4554859e9c4 button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-9675e229-885a-48a5-b26d-d4554859e9c4'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } </script> Let's also visualize the score arrays for different algorithms and the peak points fig, axes = plt.subplots(len(algorithms), 1, figsize=(12.5, len(algorithms) * 2.5 + 0.25)) for i, (cpd, alg_name) in enumerate(algorithms): axes[i].plot(scores[i]) axes[i].set_title(alg_name) axes[i].vlines(cps_true, ymin=min(scores[i]), ymax=max(scores[i]), color='black', ls='--') plt.suptitle('Change Point Detection Scores for Different Algorithms') plt.tight_layout() Ruptures #!pip install -q ruptures \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.0/1.0 MB 6.1 MB/s eta 0:00:00 We will test a Window method with an unknown number of change-points import ruptures as rpt algo = rpt.Window(width=2*50, jump=1, model='rbf').fit(X_ss) print('Training set size:', X_ss.shape[0]) print('Ruptures scores size:', algo.score.size) Training set size: 3274 Ruptures scores size: 3174 There are 100 (size of a window width) scores missing. There is also an offset, so we shall add some values to the begin and end of the scores' array. rpt_scores = np.concatenate([np.zeros(50), algo.score, np.zeros(50)]) plt.figure(figsize=(12.5, 2.5+0.25)) plt.plot(rpt_scores) plt.vlines(cps_true, ymin=min(rpt_scores), ymax=max(rpt_scores), color='black', ls='--') plt.title('Ruptures Windows score') Text(0.5, 1.0, 'Ruptures Windows score') from scipy.signal import argrelmax peaks = argrelmax(rpt_scores, order=50)[0] peaks array([ 65, 182, 265, 361, 541, 722, 902, 1082, 1262, 1442, 1545, 1623, 1803, 1986, 2165, 2339, 2535, 2705, 2885, 3088, 3214]) df.loc['Ruptures Window'] = calculate_metrics(cps_true, peaks, rpt_scores) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } precision recall AUC-PR algorithm ChangePointDetectionClassifier 0.850000 1.0 1.000000 ChangePointDetectionClassifierCV 0.772727 1.0 0.996636 ChangePointDetectionRuLSIF 0.739130 1.0 0.932255 OnlineNNRuLSIF 0.894737 1.0 0.985207 OnlineNNClassifier 0.894737 1.0 0.989269 EnergyDistanceCalculator 0.894737 1.0 1.000000 SlidingWindows 0.894737 1.0 0.993636 Ruptures Window 0.809524 1.0 1.000000 .colab-df-quickchart { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-quickchart:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-quickchart { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-quickchart:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } <script> async function quickchart(key) { const containerElement = document.querySelector('#' + key); const charts = await google.colab.kernel.invokeFunction( 'suggestCharts', [key], {}); } </script> <script> function displayQuickchartButton(domScope) { let quickchartButtonEl = domScope.querySelector('#df-32bfa8e2-88bd-4151-8193-66a119b1d939 button.colab-df-quickchart'); quickchartButtonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; } displayQuickchartButton(document); </script> <style> .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } <script> const buttonEl = document.querySelector('#df-76a39ac6-dbe4-4206-980c-2deebb889c23 button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-76a39ac6-dbe4-4206-980c-2deebb889c23'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } </script> klcpd #!pip install -q git+https://github.com/HolyBayes/klcpd Preparing metadata (setup.py) ... done Building wheel for klcpd (setup.py) ... done import torch from klcpd import KL_CPD dim = 3 device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') model = KL_CPD(dim, p_wnd_dim=50, f_wnd_dim=50).to(device) model.fit(X_ss) preds = model.predict(X_ss) plt.figure(figsize=(12.5, 2.5+0.25)) plt.plot(preds) plt.vlines(cps_true, ymin=min(preds), ymax=max(preds), color='black', ls='--') plt.title('KL-CPD score') Text(0.5, 1.0, 'KL-CPD score') peaks = argrelmax(preds, order=50)[0] peaks array([ 1, 181, 352, 516, 673, 893, 1086, 1264, 1443, 1545, 1625, 1803, 1986, 2168, 2339, 2445, 2535, 2705, 2886, 3087, 3253]) df.loc['KL-CPD'] = calculate_metrics(cps_true, peaks, preds) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } precision recall AUC-PR algorithm ChangePointDetectionClassifier 0.850000 1.0 1.000000 ChangePointDetectionClassifierCV 0.772727 1.0 0.996636 ChangePointDetectionRuLSIF 0.739130 1.0 0.932255 OnlineNNRuLSIF 0.894737 1.0 0.985207 OnlineNNClassifier 0.894737 1.0 0.989269 EnergyDistanceCalculator 0.894737 1.0 1.000000 SlidingWindows 0.894737 1.0 0.993636 Ruptures Window 0.809524 1.0 1.000000 KL-CPD 0.809524 1.0 0.965243 .colab-df-quickchart { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-quickchart:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-quickchart { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-quickchart:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } <script> async function quickchart(key) { const containerElement = document.querySelector('#' + key); const charts = await google.colab.kernel.invokeFunction( 'suggestCharts', [key], {}); } </script> <script> function displayQuickchartButton(domScope) { let quickchartButtonEl = domScope.querySelector('#df-7038e3d4-8207-4d8f-9e17-f426cd7acea6 button.colab-df-quickchart'); quickchartButtonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; } displayQuickchartButton(document); </script> <style> .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } <script> const buttonEl = document.querySelector('#df-4fae8b7c-707c-4b7f-9f84-b1311bfed5cd button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-4fae8b7c-707c-4b7f-9f84-b1311bfed5cd'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } </script>","title":"benchmarking Copy1"},{"location":"examples/benchmarking-Copy1/#benchmarking","text":"","title":"Benchmarking"},{"location":"examples/benchmarking-Copy1/#description","text":"This notebook compares the quality of algorithms in Roerich , Ruptures and klcpd libraries. We use \u201cWISDM Smartphone and Smartwatch Activity and Biometrics Dataset\u201d [1, 2], prepared by the Wireless Sensor Data Mining (WISDM) Lab in the Department of Computer and Information Science of Fordham University. The dataset includes data from the accelerometer and gyroscope sensors of a smartphone and smartwatch collected as 51 subjects performed 18 diverse activities of daily living. Each activity was performed for 3 minutes, so that each subject contributed 54 minutes of data. These activities include basic ambulation-related activities (e.g., walking, jogging, climbing stairs), hand-based activities of daily living (e.g., brushing teeth, folding clothes), and various eating activities (eating pasta, easting chips). Each subject had a smartwatch placed on his/her dominant hand and a smartphone in their pocket. The data collection was controlled by a custom-made app that ran on the smartphone and smartwatch. The sensor data was collected at a rate of 20 Hz (i.e., every 50ms) from the accelerometer and gyroscope on both the smartphone and smartwatch, yielding four total sensors. [1] Weiss, Gary. (2019). WISDM Smartphone and Smartwatch Activity and Biometrics Dataset . UCI Machine Learning Repository. [link] [2] Weiss, Gary & Yoneda, Kenichi & Hayajneh, Thaier. (2019). Smartphone and Smartwatch-Based Biometrics Using Activities of Daily Living. IEEE Access. PP. 1-1. [DOI] .","title":"Description"},{"location":"examples/benchmarking-Copy1/#data-download","text":"The dataset is stored in UCI Machine Learning Repository and available for download via this link . #!wget -q https://archive.ics.uci.edu/ml/machine-learning-databases/00507/wisdm-dataset.zip #!unzip -o -q wisdm-dataset.zip In this example, we use data from a smartwatch accelerometer collected from one subject. The data file contains the following columns: id: uniquely identifies the subject. Rand: 1600 - 1650; activity: identifies a specific activity. Range: A-S (no \u201cN\u201d value); timestamp: Linux time; x: sensor value for x axis. May be positive or negative; y: same as x but for y axis; z: same as x but for z axis; Let\u2019s download the dataset and read a sample. #!pip install -q roerich #import of basic libraries import matplotlib.pyplot as plt import pandas as pd import numpy as np import roerich import torch # helping function for reading the data def remove_char(fname): with open(fname, \"r\") as f: flist = f.readlines() flist = [s.replace(';', '') for s in flist] with open(fname[:-4] + '_debug.txt', \"w\") as f: f.writelines(flist) # path to a file in the dataset data_path = \"wisdm-dataset/raw/watch/accel/data_1600_accel_watch.txt\" remove_char(data_path) # read the data cols = [\"id\", \"activity\", \"timestamp\", \"x\", \"y\", \"z\"] data = pd.read_csv(data_path[:-4] + '_debug.txt', names=cols) data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id activity timestamp x y z 0 1600 A 90426708196641 7.091625 -0.591667 8.195502 1 1600 A 90426757696641 4.972757 -0.158317 6.696732 2 1600 A 90426807196641 3.253720 -0.191835 6.107758 3 1600 A 90426856696641 2.801216 -0.155922 5.997625 4 1600 A 90426906196641 3.770868 -1.051354 7.731027 .colab-df-quickchart { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-quickchart:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-quickchart { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-quickchart:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } <script> async function quickchart(key) { const containerElement = document.querySelector('#' + key); const charts = await google.colab.kernel.invokeFunction( 'suggestCharts', [key], {}); } </script> <script> function displayQuickchartButton(domScope) { let quickchartButtonEl = domScope.querySelector('#df-c149b3fc-b2df-4368-9c0e-c3ff5db5a86b button.colab-df-quickchart'); quickchartButtonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; } displayQuickchartButton(document); </script> <style> .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } <script> const buttonEl = document.querySelector('#df-178cb2c5-f226-4743-9fa0-0221616266ff button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-178cb2c5-f226-4743-9fa0-0221616266ff'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } </script>","title":"Data Download"},{"location":"examples/benchmarking-Copy1/#preprocessing","text":"We will use only sensor measurements and activity labels. So, let\u2019s define the input matrix X and the label vector y. We will take every 20'th observation in order to speed up the computation. #Taking only every 20th observation X = data[['x', 'y', 'z']].values[::20] y = data['activity'].values[::20] However, we do not need the activity labels by itself, but moments of the activity changes. We consider them as change points of the signal, and our goal is to detect them. # transform activity labels into change point positions def get_true_cpds(y): cps_true = [] for i in range(1, len(y)): if y[i] != y[i-1]: cps_true.append(i) return np.array(cps_true) # get true change point positions cps_true = get_true_cpds(y) Then, we preprocess the time series using StandardScaler . It is not mandatory in general case, but sometimes can be useful :). Besides this, we add some noise to our signal. This hack is a regularization technique that helps to reduce sensitivity of the change point detection methods. It decreases the number of change points within one activity. from sklearn.preprocessing import StandardScaler # scale the signal ss = StandardScaler().fit(X) X_ss = ss.transform(X) # add noise for regularization X_ss += np.random.normal(0, 0.1, X_ss.shape) roerich.display(X_ss, cps_true)","title":"Preprocessing"},{"location":"examples/benchmarking-Copy1/#comparison-of-algorithms","text":"In this section we will compare algorithms from the Roerich library with a few from other libraries ( Ruptures and klcpd )","title":"Comparison of algorithms"},{"location":"examples/benchmarking-Copy1/#roerich","text":"#Default parameters (taken from `Usage` tab in documentation) base_clf = 'mlp' base_reg = 'mlp' base_window_size = 50 base_lag_size = 50 base_periods = 1 base_n_runs = 1 base_step = 1 base_n_epochs = 10 base_n_splits = 5 base_alpha = 0.2 base_lr = 0.01 base_lam = 0.0001 base_optimizer = 'Adam' from roerich.change_point import ChangePointDetectionRuLSIF, ChangePointDetectionClassifier, \\ EnergyDistanceCalculator, ChangePointDetectionClassifierCV, \\ OnlineNNRuLSIF, OnlineNNClassifier, SlidingWindows algorithms = [(ChangePointDetectionClassifier(base_classifier=base_clf, metric='KL_sym', periods=base_periods, window_size=base_window_size, step=base_step, n_runs=base_n_runs), 'ChangePointDetectionClassifier'), (ChangePointDetectionClassifierCV(base_classifier=base_clf, metric='KL_sym', periods=base_periods, window_size=base_window_size, step=base_step, n_splits=base_n_splits), 'ChangePointDetectionClassifierCV'), (ChangePointDetectionRuLSIF(base_regressor=base_reg, metric='PE', periods=base_periods, window_size=base_window_size, step=base_step, n_runs=base_n_runs), 'ChangePointDetectionRuLSIF'), (OnlineNNRuLSIF(alpha=base_alpha, net='auto', scaler='auto', periods=base_periods, window_size=1, #algorithm's feature lag_size=base_lag_size, step=base_step, n_epochs=base_n_epochs, lr=base_lr, lam=base_lam, optimizer=base_optimizer), 'OnlineNNRuLSIF'), (OnlineNNClassifier(net='auto', scaler='auto', metric='KL_sym', periods=base_periods, window_size=1, #algorithm's feature lag_size=base_lag_size, step=base_step, n_epochs=base_n_epochs, lr=base_lr, lam=base_lam, optimizer=base_optimizer), 'OnlineNNClassifier'), (EnergyDistanceCalculator(window_size=base_window_size, step=base_step), 'EnergyDistanceCalculator'), (SlidingWindows(periods=base_periods, window_size=base_window_size, step=base_step, n_runs=base_n_runs), 'SlidingWindows')] We will write a function that takes an array of true change-points, predicted ones and the scores and returns an array of three metrics: precision, recall and AUC-PR. from roerich.metrics import precision_recall_scores, precision_recall_curve, auc_score def calculate_metrics(cps_true, cps_pred, score): precision, recall = precision_recall_scores(cps_true, cps_pred, window=base_window_size) results = [precision, recall] thr, precision, recall = precision_recall_curve(cps_true, cps_pred, score[cps_pred], window=base_window_size) results.append(auc_score(thr, precision, recall)) return results The results will be stored in a dataframe: the columns are different metrics and the rows are the results for different algorithms. df = pd.DataFrame(columns=['algorithm', 'precision', 'recall', 'AUC-PR']) df.set_index('algorithm', inplace=True) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } precision recall AUC-PR algorithm .colab-df-quickchart { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-quickchart:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-quickchart { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-quickchart:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } <script> async function quickchart(key) { const containerElement = document.querySelector('#' + key); const charts = await google.colab.kernel.invokeFunction( 'suggestCharts', [key], {}); } </script> <script> function displayQuickchartButton(domScope) { let quickchartButtonEl = domScope.querySelector('#df-d5b27179-5a0e-4c6c-9088-a6ac6a22b560 button.colab-df-quickchart'); quickchartButtonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; } displayQuickchartButton(document); </script> <style> .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } <script> const buttonEl = document.querySelector('#df-61cb2dd7-9a49-4e90-807e-15fd8f4d22a3 button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-61cb2dd7-9a49-4e90-807e-15fd8f4d22a3'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } </script> #Iterating through all the algorithms scores = [] #We store scores to plot them later for cpd, alg_name in algorithms: score, cps_pred = cpd.predict(X_ss) df.loc[alg_name] = calculate_metrics(cps_true, cps_pred, score) scores.append(score) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } precision recall AUC-PR algorithm ChangePointDetectionClassifier 0.850000 1.0 1.000000 ChangePointDetectionClassifierCV 0.772727 1.0 0.996636 ChangePointDetectionRuLSIF 0.739130 1.0 0.932255 OnlineNNRuLSIF 0.894737 1.0 0.985207 OnlineNNClassifier 0.894737 1.0 0.989269 EnergyDistanceCalculator 0.894737 1.0 1.000000 SlidingWindows 0.894737 1.0 0.993636 .colab-df-quickchart { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-quickchart:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-quickchart { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-quickchart:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } <script> async function quickchart(key) { const containerElement = document.querySelector('#' + key); const charts = await google.colab.kernel.invokeFunction( 'suggestCharts', [key], {}); } </script> <script> function displayQuickchartButton(domScope) { let quickchartButtonEl = domScope.querySelector('#df-8865d658-9955-4523-b629-8a9113faa452 button.colab-df-quickchart'); quickchartButtonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; } displayQuickchartButton(document); </script> <style> .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } <script> const buttonEl = document.querySelector('#df-9675e229-885a-48a5-b26d-d4554859e9c4 button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-9675e229-885a-48a5-b26d-d4554859e9c4'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } </script> Let's also visualize the score arrays for different algorithms and the peak points fig, axes = plt.subplots(len(algorithms), 1, figsize=(12.5, len(algorithms) * 2.5 + 0.25)) for i, (cpd, alg_name) in enumerate(algorithms): axes[i].plot(scores[i]) axes[i].set_title(alg_name) axes[i].vlines(cps_true, ymin=min(scores[i]), ymax=max(scores[i]), color='black', ls='--') plt.suptitle('Change Point Detection Scores for Different Algorithms') plt.tight_layout()","title":"Roerich"},{"location":"examples/benchmarking-Copy1/#ruptures","text":"#!pip install -q ruptures \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.0/1.0 MB 6.1 MB/s eta 0:00:00 We will test a Window method with an unknown number of change-points import ruptures as rpt algo = rpt.Window(width=2*50, jump=1, model='rbf').fit(X_ss) print('Training set size:', X_ss.shape[0]) print('Ruptures scores size:', algo.score.size) Training set size: 3274 Ruptures scores size: 3174 There are 100 (size of a window width) scores missing. There is also an offset, so we shall add some values to the begin and end of the scores' array. rpt_scores = np.concatenate([np.zeros(50), algo.score, np.zeros(50)]) plt.figure(figsize=(12.5, 2.5+0.25)) plt.plot(rpt_scores) plt.vlines(cps_true, ymin=min(rpt_scores), ymax=max(rpt_scores), color='black', ls='--') plt.title('Ruptures Windows score') Text(0.5, 1.0, 'Ruptures Windows score') from scipy.signal import argrelmax peaks = argrelmax(rpt_scores, order=50)[0] peaks array([ 65, 182, 265, 361, 541, 722, 902, 1082, 1262, 1442, 1545, 1623, 1803, 1986, 2165, 2339, 2535, 2705, 2885, 3088, 3214]) df.loc['Ruptures Window'] = calculate_metrics(cps_true, peaks, rpt_scores) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } precision recall AUC-PR algorithm ChangePointDetectionClassifier 0.850000 1.0 1.000000 ChangePointDetectionClassifierCV 0.772727 1.0 0.996636 ChangePointDetectionRuLSIF 0.739130 1.0 0.932255 OnlineNNRuLSIF 0.894737 1.0 0.985207 OnlineNNClassifier 0.894737 1.0 0.989269 EnergyDistanceCalculator 0.894737 1.0 1.000000 SlidingWindows 0.894737 1.0 0.993636 Ruptures Window 0.809524 1.0 1.000000 .colab-df-quickchart { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-quickchart:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-quickchart { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-quickchart:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } <script> async function quickchart(key) { const containerElement = document.querySelector('#' + key); const charts = await google.colab.kernel.invokeFunction( 'suggestCharts', [key], {}); } </script> <script> function displayQuickchartButton(domScope) { let quickchartButtonEl = domScope.querySelector('#df-32bfa8e2-88bd-4151-8193-66a119b1d939 button.colab-df-quickchart'); quickchartButtonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; } displayQuickchartButton(document); </script> <style> .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } <script> const buttonEl = document.querySelector('#df-76a39ac6-dbe4-4206-980c-2deebb889c23 button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-76a39ac6-dbe4-4206-980c-2deebb889c23'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } </script>","title":"Ruptures"},{"location":"examples/benchmarking-Copy1/#klcpd","text":"#!pip install -q git+https://github.com/HolyBayes/klcpd Preparing metadata (setup.py) ... done Building wheel for klcpd (setup.py) ... done import torch from klcpd import KL_CPD dim = 3 device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') model = KL_CPD(dim, p_wnd_dim=50, f_wnd_dim=50).to(device) model.fit(X_ss) preds = model.predict(X_ss) plt.figure(figsize=(12.5, 2.5+0.25)) plt.plot(preds) plt.vlines(cps_true, ymin=min(preds), ymax=max(preds), color='black', ls='--') plt.title('KL-CPD score') Text(0.5, 1.0, 'KL-CPD score') peaks = argrelmax(preds, order=50)[0] peaks array([ 1, 181, 352, 516, 673, 893, 1086, 1264, 1443, 1545, 1625, 1803, 1986, 2168, 2339, 2445, 2535, 2705, 2886, 3087, 3253]) df.loc['KL-CPD'] = calculate_metrics(cps_true, peaks, preds) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } precision recall AUC-PR algorithm ChangePointDetectionClassifier 0.850000 1.0 1.000000 ChangePointDetectionClassifierCV 0.772727 1.0 0.996636 ChangePointDetectionRuLSIF 0.739130 1.0 0.932255 OnlineNNRuLSIF 0.894737 1.0 0.985207 OnlineNNClassifier 0.894737 1.0 0.989269 EnergyDistanceCalculator 0.894737 1.0 1.000000 SlidingWindows 0.894737 1.0 0.993636 Ruptures Window 0.809524 1.0 1.000000 KL-CPD 0.809524 1.0 0.965243 .colab-df-quickchart { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-quickchart:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-quickchart { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-quickchart:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } <script> async function quickchart(key) { const containerElement = document.querySelector('#' + key); const charts = await google.colab.kernel.invokeFunction( 'suggestCharts', [key], {}); } </script> <script> function displayQuickchartButton(domScope) { let quickchartButtonEl = domScope.querySelector('#df-7038e3d4-8207-4d8f-9e17-f426cd7acea6 button.colab-df-quickchart'); quickchartButtonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; } displayQuickchartButton(document); </script> <style> .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } <script> const buttonEl = document.querySelector('#df-4fae8b7c-707c-4b7f-9f84-b1311bfed5cd button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-4fae8b7c-707c-4b7f-9f84-b1311bfed5cd'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } </script>","title":"klcpd"},{"location":"examples/benchmarking/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Benchmarking Description This notebook compares the quality of algorithms in Roerich , Ruptures and klcpd libraries. We use \u201cWISDM Smartphone and Smartwatch Activity and Biometrics Dataset\u201d [1, 2], prepared by the Wireless Sensor Data Mining (WISDM) Lab in the Department of Computer and Information Science of Fordham University. The dataset includes data from the accelerometer and gyroscope sensors of a smartphone and smartwatch collected as 51 subjects performed 18 diverse activities of daily living. Each activity was performed for 3 minutes, so that each subject contributed 54 minutes of data. These activities include basic ambulation-related activities (e.g., walking, jogging, climbing stairs), hand-based activities of daily living (e.g., brushing teeth, folding clothes), and various eating activities (eating pasta, easting chips). Each subject had a smartwatch placed on his/her dominant hand and a smartphone in their pocket. The data collection was controlled by a custom-made app that ran on the smartphone and smartwatch. The sensor data was collected at a rate of 20 Hz (i.e., every 50ms) from the accelerometer and gyroscope on both the smartphone and smartwatch, yielding four total sensors. [1] Weiss, Gary. (2019). WISDM Smartphone and Smartwatch Activity and Biometrics Dataset . UCI Machine Learning Repository. [link] [2] Weiss, Gary & Yoneda, Kenichi & Hayajneh, Thaier. (2019). Smartphone and Smartwatch-Based Biometrics Using Activities of Daily Living. IEEE Access. PP. 1-1. [DOI] . Data Download The dataset is stored in UCI Machine Learning Repository and available for download via this link . #!wget -q https://archive.ics.uci.edu/ml/machine-learning-databases/00507/wisdm-dataset.zip #!unzip -o -q wisdm-dataset.zip In this example, we use data from a smartwatch accelerometer collected from one subject. The data file contains the following columns: id: uniquely identifies the subject. Rand: 1600 - 1650; activity: identifies a specific activity. Range: A-S (no \u201cN\u201d value); timestamp: Linux time; x: sensor value for x axis. May be positive or negative; y: same as x but for y axis; z: same as x but for z axis; Let\u2019s download the dataset and read a sample. #!pip install -q roerich #import of basic libraries import matplotlib.pyplot as plt import pandas as pd import numpy as np import roerich import torch # helping function for reading the data def remove_char(fname): with open(fname, \"r\") as f: flist = f.readlines() flist = [s.replace(';', '') for s in flist] with open(fname[:-4] + '_debug.txt', \"w\") as f: f.writelines(flist) # path to a file in the dataset data_path = \"wisdm-dataset/raw/watch/accel/data_1600_accel_watch.txt\" remove_char(data_path) # read the data cols = [\"id\", \"activity\", \"timestamp\", \"x\", \"y\", \"z\"] data = pd.read_csv(data_path[:-4] + '_debug.txt', names=cols) data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id activity timestamp x y z 0 1600 A 90426708196641 7.091625 -0.591667 8.195502 1 1600 A 90426757696641 4.972757 -0.158317 6.696732 2 1600 A 90426807196641 3.253720 -0.191835 6.107758 3 1600 A 90426856696641 2.801216 -0.155922 5.997625 4 1600 A 90426906196641 3.770868 -1.051354 7.731027 Preprocessing We will use only sensor measurements and activity labels. So, let\u2019s define the input matrix X and the label vector y. We will take every 20'th observation in order to speed up the computation. #Taking only every 20th observation X = data[['x', 'y', 'z']].values[::20] y = data['activity'].values[::20] However, we do not need the activity labels by itself, but moments of the activity changes. We consider them as change points of the signal, and our goal is to detect them. # transform activity labels into change point positions def get_true_cpds(y): cps_true = [] for i in range(1, len(y)): if y[i] != y[i-1]: cps_true.append(i) return np.array(cps_true) # get true change point positions cps_true = get_true_cpds(y) Then, we preprocess the time series using StandardScaler . It is not mandatory in general case, but sometimes can be useful :). Besides this, we add some noise to our signal. This hack is a regularization technique that helps to reduce sensitivity of the change point detection methods. It decreases the number of change points within one activity. from sklearn.preprocessing import StandardScaler # scale the signal ss = StandardScaler().fit(X) X_ss = ss.transform(X) # add noise for regularization X_ss += np.random.normal(0, 0.1, X_ss.shape) roerich.display(X_ss, cps_true) Comparison of algorithms In this section we will compare algorithms from the Roerich library with a few from other libraries ( Ruptures and klcpd ) Roerich #Default parameters (taken from `Usage` tab in documentation) base_clf = 'mlp' base_reg = 'mlp' base_window_size = 50 base_lag_size = 50 base_periods = 1 base_n_runs = 1 base_step = 1 base_n_epochs = 10 base_n_splits = 5 base_alpha = 0.2 base_lr = 0.01 base_lam = 0.0001 base_optimizer = 'Adam' from roerich.change_point import ChangePointDetectionRuLSIF, ChangePointDetectionClassifier, \\ EnergyDistanceCalculator, ChangePointDetectionClassifierCV, \\ OnlineNNRuLSIF, OnlineNNClassifier, SlidingWindows algorithms = [(ChangePointDetectionClassifier(base_classifier=base_clf, metric='KL_sym', periods=base_periods, window_size=base_window_size, step=base_step, n_runs=base_n_runs), 'ChangePointDetectionClassifier'), (ChangePointDetectionClassifierCV(base_classifier=base_clf, metric='KL_sym', periods=base_periods, window_size=base_window_size, step=base_step, n_splits=base_n_splits), 'ChangePointDetectionClassifierCV'), (ChangePointDetectionRuLSIF(base_regressor=base_reg, metric='PE', periods=base_periods, window_size=base_window_size, step=base_step, n_runs=base_n_runs), 'ChangePointDetectionRuLSIF'), (OnlineNNRuLSIF(alpha=base_alpha, net='auto', scaler='auto', periods=base_periods, window_size=1, #algorithm's feature lag_size=base_lag_size, step=base_step, n_epochs=base_n_epochs, lr=base_lr, lam=base_lam, optimizer=base_optimizer), 'OnlineNNRuLSIF'), (OnlineNNClassifier(net='auto', scaler='auto', metric='KL_sym', periods=base_periods, window_size=1, #algorithm's feature lag_size=base_lag_size, step=base_step, n_epochs=base_n_epochs, lr=base_lr, lam=base_lam, optimizer=base_optimizer), 'OnlineNNClassifier'), (EnergyDistanceCalculator(window_size=base_window_size, step=base_step), 'EnergyDistanceCalculator'), (SlidingWindows(periods=base_periods, window_size=base_window_size, step=base_step, n_runs=base_n_runs), 'SlidingWindows')] We will write a function that takes an array of true change-points, predicted ones and the scores and returns an array of three metrics: precision, recall and AUC-PR. from roerich.metrics import precision_recall_scores, precision_recall_curve, auc_score def calculate_metrics(cps_true, cps_pred, score): precision, recall = precision_recall_scores(cps_true, cps_pred, window=base_window_size) results = [precision, recall] thr, precision, recall = precision_recall_curve(cps_true, cps_pred, score[cps_pred], window=base_window_size) results.append(auc_score(thr, precision, recall)) return results The results will be stored in a dataframe: the columns are different metrics and the rows are the results for different algorithms. df = pd.DataFrame(columns=['algorithm', 'precision', 'recall', 'AUC-PR']) df.set_index('algorithm', inplace=True) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } precision recall AUC-PR algorithm #Iterating through all the algorithms scores = [] #We store scores to plot them later for cpd, alg_name in algorithms: score, cps_pred = cpd.predict(X_ss) df.loc[alg_name] = calculate_metrics(cps_true, cps_pred, score) scores.append(score) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } precision recall AUC-PR algorithm ChangePointDetectionClassifier 0.850000 1.000000 0.996636 ChangePointDetectionClassifierCV 0.761905 0.941176 0.963585 ChangePointDetectionRuLSIF 0.809524 1.000000 0.996636 OnlineNNRuLSIF 0.944444 1.000000 0.985207 OnlineNNClassifier 0.894737 1.000000 0.989269 EnergyDistanceCalculator 0.894737 1.000000 1.000000 SlidingWindows 0.894737 1.000000 0.996636 Let's also visualize the score arrays for different algorithms and the peak points fig, axes = plt.subplots(len(algorithms), 1, figsize=(12.5, len(algorithms) * 2.5 + 0.25)) for i, (cpd, alg_name) in enumerate(algorithms): axes[i].plot(scores[i]) axes[i].set_title(alg_name) axes[i].vlines(cps_true, ymin=min(scores[i]), ymax=max(scores[i]), color='black', ls='--') plt.suptitle('Change Point Detection Scores for Different Algorithms') plt.tight_layout() Ruptures #!pip install -q ruptures We will test a Window method with an unknown number of change-points import ruptures as rpt algo = rpt.Window(width=2*50, jump=1, model='rbf').fit(X_ss) print('Training set size:', X_ss.shape[0]) print('Ruptures scores size:', algo.score.size) Training set size: 3274 Ruptures scores size: 3174 There are 100 (size of a window width) scores missing. There is also an offset, so we shall add some values to the begin and end of the scores' array. rpt_scores = np.concatenate([np.zeros(50), algo.score, np.zeros(50)]) plt.figure(figsize=(12.5, 2.5+0.25)) plt.plot(rpt_scores) plt.vlines(cps_true, ymin=min(rpt_scores), ymax=max(rpt_scores), color='black', ls='--') plt.title('Ruptures Windows score') Text(0.5, 1.0, 'Ruptures Windows score') from scipy.signal import argrelmax peaks = argrelmax(rpt_scores, order=50)[0] peaks array([ 69, 181, 361, 541, 722, 902, 1082, 1262, 1442, 1545, 1623, 1803, 1986, 2165, 2339, 2535, 2705, 2885, 3088, 3214]) df.loc['Ruptures Window'] = calculate_metrics(cps_true, peaks, rpt_scores) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } precision recall AUC-PR algorithm ChangePointDetectionClassifier 0.850000 1.000000 0.996636 ChangePointDetectionClassifierCV 0.761905 0.941176 0.963585 ChangePointDetectionRuLSIF 0.809524 1.000000 0.996636 OnlineNNRuLSIF 0.944444 1.000000 0.985207 OnlineNNClassifier 0.894737 1.000000 0.989269 EnergyDistanceCalculator 0.894737 1.000000 1.000000 SlidingWindows 0.894737 1.000000 0.996636 Ruptures Window 0.850000 1.000000 1.000000 klcpd #!pip install -q git+https://github.com/HolyBayes/klcpd import torch from klcpd import KL_CPD dim = 3 device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') model = KL_CPD(dim, p_wnd_dim=50, f_wnd_dim=50).to(device) model.fit(X_ss, epoches=10) preds = model.predict(X_ss) plt.figure(figsize=(12.5, 2.5+0.25)) plt.plot(preds) plt.vlines(cps_true, ymin=min(preds), ymax=max(preds), color='black', ls='--') plt.title('KL-CPD score') Text(0.5, 1.0, 'KL-CPD score') peaks = argrelmax(preds, order=50)[0] peaks array([ 182, 269, 365, 541, 724, 901, 1087, 1261, 1443, 1545, 1626, 1802, 1986, 2165, 2347, 2419, 2582, 2706, 2886, 3087]) df.loc['KL-CPD'] = calculate_metrics(cps_true, peaks, preds) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } precision recall AUC-PR algorithm ChangePointDetectionClassifier 0.850000 1.000000 0.996636 ChangePointDetectionClassifierCV 0.761905 0.941176 0.963585 ChangePointDetectionRuLSIF 0.809524 1.000000 0.996636 OnlineNNRuLSIF 0.944444 1.000000 0.985207 OnlineNNClassifier 0.894737 1.000000 0.989269 EnergyDistanceCalculator 0.894737 1.000000 1.000000 SlidingWindows 0.894737 1.000000 0.996636 Ruptures Window 0.850000 1.000000 1.000000 KL-CPD 0.800000 0.941176 0.942539","title":"Benchmarking"},{"location":"examples/benchmarking/#benchmarking","text":"","title":"Benchmarking"},{"location":"examples/benchmarking/#description","text":"This notebook compares the quality of algorithms in Roerich , Ruptures and klcpd libraries. We use \u201cWISDM Smartphone and Smartwatch Activity and Biometrics Dataset\u201d [1, 2], prepared by the Wireless Sensor Data Mining (WISDM) Lab in the Department of Computer and Information Science of Fordham University. The dataset includes data from the accelerometer and gyroscope sensors of a smartphone and smartwatch collected as 51 subjects performed 18 diverse activities of daily living. Each activity was performed for 3 minutes, so that each subject contributed 54 minutes of data. These activities include basic ambulation-related activities (e.g., walking, jogging, climbing stairs), hand-based activities of daily living (e.g., brushing teeth, folding clothes), and various eating activities (eating pasta, easting chips). Each subject had a smartwatch placed on his/her dominant hand and a smartphone in their pocket. The data collection was controlled by a custom-made app that ran on the smartphone and smartwatch. The sensor data was collected at a rate of 20 Hz (i.e., every 50ms) from the accelerometer and gyroscope on both the smartphone and smartwatch, yielding four total sensors. [1] Weiss, Gary. (2019). WISDM Smartphone and Smartwatch Activity and Biometrics Dataset . UCI Machine Learning Repository. [link] [2] Weiss, Gary & Yoneda, Kenichi & Hayajneh, Thaier. (2019). Smartphone and Smartwatch-Based Biometrics Using Activities of Daily Living. IEEE Access. PP. 1-1. [DOI] .","title":"Description"},{"location":"examples/benchmarking/#data-download","text":"The dataset is stored in UCI Machine Learning Repository and available for download via this link . #!wget -q https://archive.ics.uci.edu/ml/machine-learning-databases/00507/wisdm-dataset.zip #!unzip -o -q wisdm-dataset.zip In this example, we use data from a smartwatch accelerometer collected from one subject. The data file contains the following columns: id: uniquely identifies the subject. Rand: 1600 - 1650; activity: identifies a specific activity. Range: A-S (no \u201cN\u201d value); timestamp: Linux time; x: sensor value for x axis. May be positive or negative; y: same as x but for y axis; z: same as x but for z axis; Let\u2019s download the dataset and read a sample. #!pip install -q roerich #import of basic libraries import matplotlib.pyplot as plt import pandas as pd import numpy as np import roerich import torch # helping function for reading the data def remove_char(fname): with open(fname, \"r\") as f: flist = f.readlines() flist = [s.replace(';', '') for s in flist] with open(fname[:-4] + '_debug.txt', \"w\") as f: f.writelines(flist) # path to a file in the dataset data_path = \"wisdm-dataset/raw/watch/accel/data_1600_accel_watch.txt\" remove_char(data_path) # read the data cols = [\"id\", \"activity\", \"timestamp\", \"x\", \"y\", \"z\"] data = pd.read_csv(data_path[:-4] + '_debug.txt', names=cols) data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id activity timestamp x y z 0 1600 A 90426708196641 7.091625 -0.591667 8.195502 1 1600 A 90426757696641 4.972757 -0.158317 6.696732 2 1600 A 90426807196641 3.253720 -0.191835 6.107758 3 1600 A 90426856696641 2.801216 -0.155922 5.997625 4 1600 A 90426906196641 3.770868 -1.051354 7.731027","title":"Data Download"},{"location":"examples/benchmarking/#preprocessing","text":"We will use only sensor measurements and activity labels. So, let\u2019s define the input matrix X and the label vector y. We will take every 20'th observation in order to speed up the computation. #Taking only every 20th observation X = data[['x', 'y', 'z']].values[::20] y = data['activity'].values[::20] However, we do not need the activity labels by itself, but moments of the activity changes. We consider them as change points of the signal, and our goal is to detect them. # transform activity labels into change point positions def get_true_cpds(y): cps_true = [] for i in range(1, len(y)): if y[i] != y[i-1]: cps_true.append(i) return np.array(cps_true) # get true change point positions cps_true = get_true_cpds(y) Then, we preprocess the time series using StandardScaler . It is not mandatory in general case, but sometimes can be useful :). Besides this, we add some noise to our signal. This hack is a regularization technique that helps to reduce sensitivity of the change point detection methods. It decreases the number of change points within one activity. from sklearn.preprocessing import StandardScaler # scale the signal ss = StandardScaler().fit(X) X_ss = ss.transform(X) # add noise for regularization X_ss += np.random.normal(0, 0.1, X_ss.shape) roerich.display(X_ss, cps_true)","title":"Preprocessing"},{"location":"examples/benchmarking/#comparison-of-algorithms","text":"In this section we will compare algorithms from the Roerich library with a few from other libraries ( Ruptures and klcpd )","title":"Comparison of algorithms"},{"location":"examples/benchmarking/#roerich","text":"#Default parameters (taken from `Usage` tab in documentation) base_clf = 'mlp' base_reg = 'mlp' base_window_size = 50 base_lag_size = 50 base_periods = 1 base_n_runs = 1 base_step = 1 base_n_epochs = 10 base_n_splits = 5 base_alpha = 0.2 base_lr = 0.01 base_lam = 0.0001 base_optimizer = 'Adam' from roerich.change_point import ChangePointDetectionRuLSIF, ChangePointDetectionClassifier, \\ EnergyDistanceCalculator, ChangePointDetectionClassifierCV, \\ OnlineNNRuLSIF, OnlineNNClassifier, SlidingWindows algorithms = [(ChangePointDetectionClassifier(base_classifier=base_clf, metric='KL_sym', periods=base_periods, window_size=base_window_size, step=base_step, n_runs=base_n_runs), 'ChangePointDetectionClassifier'), (ChangePointDetectionClassifierCV(base_classifier=base_clf, metric='KL_sym', periods=base_periods, window_size=base_window_size, step=base_step, n_splits=base_n_splits), 'ChangePointDetectionClassifierCV'), (ChangePointDetectionRuLSIF(base_regressor=base_reg, metric='PE', periods=base_periods, window_size=base_window_size, step=base_step, n_runs=base_n_runs), 'ChangePointDetectionRuLSIF'), (OnlineNNRuLSIF(alpha=base_alpha, net='auto', scaler='auto', periods=base_periods, window_size=1, #algorithm's feature lag_size=base_lag_size, step=base_step, n_epochs=base_n_epochs, lr=base_lr, lam=base_lam, optimizer=base_optimizer), 'OnlineNNRuLSIF'), (OnlineNNClassifier(net='auto', scaler='auto', metric='KL_sym', periods=base_periods, window_size=1, #algorithm's feature lag_size=base_lag_size, step=base_step, n_epochs=base_n_epochs, lr=base_lr, lam=base_lam, optimizer=base_optimizer), 'OnlineNNClassifier'), (EnergyDistanceCalculator(window_size=base_window_size, step=base_step), 'EnergyDistanceCalculator'), (SlidingWindows(periods=base_periods, window_size=base_window_size, step=base_step, n_runs=base_n_runs), 'SlidingWindows')] We will write a function that takes an array of true change-points, predicted ones and the scores and returns an array of three metrics: precision, recall and AUC-PR. from roerich.metrics import precision_recall_scores, precision_recall_curve, auc_score def calculate_metrics(cps_true, cps_pred, score): precision, recall = precision_recall_scores(cps_true, cps_pred, window=base_window_size) results = [precision, recall] thr, precision, recall = precision_recall_curve(cps_true, cps_pred, score[cps_pred], window=base_window_size) results.append(auc_score(thr, precision, recall)) return results The results will be stored in a dataframe: the columns are different metrics and the rows are the results for different algorithms. df = pd.DataFrame(columns=['algorithm', 'precision', 'recall', 'AUC-PR']) df.set_index('algorithm', inplace=True) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } precision recall AUC-PR algorithm #Iterating through all the algorithms scores = [] #We store scores to plot them later for cpd, alg_name in algorithms: score, cps_pred = cpd.predict(X_ss) df.loc[alg_name] = calculate_metrics(cps_true, cps_pred, score) scores.append(score) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } precision recall AUC-PR algorithm ChangePointDetectionClassifier 0.850000 1.000000 0.996636 ChangePointDetectionClassifierCV 0.761905 0.941176 0.963585 ChangePointDetectionRuLSIF 0.809524 1.000000 0.996636 OnlineNNRuLSIF 0.944444 1.000000 0.985207 OnlineNNClassifier 0.894737 1.000000 0.989269 EnergyDistanceCalculator 0.894737 1.000000 1.000000 SlidingWindows 0.894737 1.000000 0.996636 Let's also visualize the score arrays for different algorithms and the peak points fig, axes = plt.subplots(len(algorithms), 1, figsize=(12.5, len(algorithms) * 2.5 + 0.25)) for i, (cpd, alg_name) in enumerate(algorithms): axes[i].plot(scores[i]) axes[i].set_title(alg_name) axes[i].vlines(cps_true, ymin=min(scores[i]), ymax=max(scores[i]), color='black', ls='--') plt.suptitle('Change Point Detection Scores for Different Algorithms') plt.tight_layout()","title":"Roerich"},{"location":"examples/benchmarking/#ruptures","text":"#!pip install -q ruptures We will test a Window method with an unknown number of change-points import ruptures as rpt algo = rpt.Window(width=2*50, jump=1, model='rbf').fit(X_ss) print('Training set size:', X_ss.shape[0]) print('Ruptures scores size:', algo.score.size) Training set size: 3274 Ruptures scores size: 3174 There are 100 (size of a window width) scores missing. There is also an offset, so we shall add some values to the begin and end of the scores' array. rpt_scores = np.concatenate([np.zeros(50), algo.score, np.zeros(50)]) plt.figure(figsize=(12.5, 2.5+0.25)) plt.plot(rpt_scores) plt.vlines(cps_true, ymin=min(rpt_scores), ymax=max(rpt_scores), color='black', ls='--') plt.title('Ruptures Windows score') Text(0.5, 1.0, 'Ruptures Windows score') from scipy.signal import argrelmax peaks = argrelmax(rpt_scores, order=50)[0] peaks array([ 69, 181, 361, 541, 722, 902, 1082, 1262, 1442, 1545, 1623, 1803, 1986, 2165, 2339, 2535, 2705, 2885, 3088, 3214]) df.loc['Ruptures Window'] = calculate_metrics(cps_true, peaks, rpt_scores) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } precision recall AUC-PR algorithm ChangePointDetectionClassifier 0.850000 1.000000 0.996636 ChangePointDetectionClassifierCV 0.761905 0.941176 0.963585 ChangePointDetectionRuLSIF 0.809524 1.000000 0.996636 OnlineNNRuLSIF 0.944444 1.000000 0.985207 OnlineNNClassifier 0.894737 1.000000 0.989269 EnergyDistanceCalculator 0.894737 1.000000 1.000000 SlidingWindows 0.894737 1.000000 0.996636 Ruptures Window 0.850000 1.000000 1.000000","title":"Ruptures"},{"location":"examples/benchmarking/#klcpd","text":"#!pip install -q git+https://github.com/HolyBayes/klcpd import torch from klcpd import KL_CPD dim = 3 device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') model = KL_CPD(dim, p_wnd_dim=50, f_wnd_dim=50).to(device) model.fit(X_ss, epoches=10) preds = model.predict(X_ss) plt.figure(figsize=(12.5, 2.5+0.25)) plt.plot(preds) plt.vlines(cps_true, ymin=min(preds), ymax=max(preds), color='black', ls='--') plt.title('KL-CPD score') Text(0.5, 1.0, 'KL-CPD score') peaks = argrelmax(preds, order=50)[0] peaks array([ 182, 269, 365, 541, 724, 901, 1087, 1261, 1443, 1545, 1626, 1802, 1986, 2165, 2347, 2419, 2582, 2706, 2886, 3087]) df.loc['KL-CPD'] = calculate_metrics(cps_true, peaks, preds) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } precision recall AUC-PR algorithm ChangePointDetectionClassifier 0.850000 1.000000 0.996636 ChangePointDetectionClassifierCV 0.761905 0.941176 0.963585 ChangePointDetectionRuLSIF 0.809524 1.000000 0.996636 OnlineNNRuLSIF 0.944444 1.000000 0.985207 OnlineNNClassifier 0.894737 1.000000 0.989269 EnergyDistanceCalculator 0.894737 1.000000 1.000000 SlidingWindows 0.894737 1.000000 0.996636 Ruptures Window 0.850000 1.000000 1.000000 KL-CPD 0.800000 0.941176 0.942539","title":"klcpd"},{"location":"examples/demo/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Basics Generate signal In this example, we consider basic usages for change point detection methods in roerich library. Let\u2019s generate a signal with different regimes, or, in other words, with different distributions of observations. The goal is to determine all change points that correspond to the distribution changes. We also can consider this task as the signal segmentation, where each segment corresponds to one regime of the signal. import roerich import roerich.algorithms import roerich.metrics import numpy as np # generate a time series with change points X, cps_true = roerich.generate_dataset(period=200, N_tot=2000) ChangePointDetectionClassifier The first method we use here is ChangePointDetectionClassifier . It is based on binary classifiers in machine learning. A classifier takes two parts of the signal with window_size observations and separates them into two classes. Then, it uses predictions to estimate the probability density ratio for these observations and calculate the change point score between the windows. The method refits the classifier on each new pair of windows. from roerich.algorithms import ChangePointDetectionClassifier from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis # sklearn-like binary classifier clf = QuadraticDiscriminantAnalysis() # change points detection cpd = ChangePointDetectionClassifier(base_classifier=clf, metric='KL_sym', periods=1, window_size=100, step=1, n_runs=1) score, cps_pred = cpd.predict(X) # visualization roerich.display(X, cps_true, score, cps_pred) Now, let\u2019s measure the quality of change point detection using Precision and Recall metrics. # metrics precision, recall = roerich.metrics.precision_recall_scores(cps_true, cps_pred, window=20) auc = roerich.metrics.pr_auc(cps_true, cps_pred, score[cps_pred], window=20) print('Precision: ', precision) print('Recall: ', recall) print('PR AUC: ', auc) Precision: 1.0 Recall: 1.0 PR AUC: 1.0 The ChangePointDetectionClassifier method can take any sklearn-like binary classifier. For example, you use your favorite Neural Network or Gradient Boosting over Decision Trees algorithm. The following example uses a shallow Neural Network implemented in PyTroch as a base classifier for change point detection. # pytorch NN classifier clf = roerich.algorithms.NNClassifier(n_hidden=10, n_epochs=10, batch_size=64, lr=0.1, l2=0.0) # detection cpd = ChangePointDetectionClassifier(base_classifier=clf, metric='KL_sym', periods=1, window_size=100, step=1, n_runs=1) score, cps_pred = cpd.predict(X) cps_pred array([ 194, 399, 598, 803, 1001, 1199, 1399, 1598, 1800]) ChangePointDetectionRuLSIF Similarly, ChangePointDetectionRuLSIF method also estimates probability density ratio between two windows of the signal. However, it does it using regression models with the RuLSIF loss function. They directly predict the ratios without learning individual distributions. Roerich provides two following RuLSIF models. NNRuLSIFRegressor model is a shallow Neural Network implemented in PyTorch. And GBDTRuLSIFRegressor is a Gradient Boosting over Regression Trees. The following examples show how to use them for change point detection. from roerich.algorithms import NNRuLSIFRegressor, ChangePointDetectionRuLSIF # pytorch NN regressor with RuLSIF loss function reg = NNRuLSIFRegressor(n_hidden=10, n_epochs=10, batch_size=64, lr=0.1, l2=0.0, alpha=0.05) # detection cpd = ChangePointDetectionRuLSIF(reg, metric='PE', periods=1, window_size=100, step=1, n_runs=1) score, cps_pred = cpd.predict(X) # visualization roerich.display(X, cps_true, score, cps_pred) from roerich.algorithms import GBDTRuLSIFRegressor # regressor with RuLSIF loss function reg = GBDTRuLSIFRegressor(n_estimators=10, max_depth=2) # detection cpd = ChangePointDetectionRuLSIF(reg, metric='PE', periods=1, window_size=100, step=1, n_runs=1) score, cps_pred = cpd.predict(X) cps_pred array([ 206, 400, 596, 798, 1001, 1201, 1401, 1600, 1799]) OnlineNNClassifier OnlineNNClassifier is based on the same principles as the previous methods. But it uses a single Neural Network to detect change points in the whole time series. The network scans the signal point by point ( window_size=1 ) and updates its weights online. It estimates the probability density ratio between distant ( lag_size=100 ) observations of the signal. In result, the method is faster than previous algorithms, because it fits the network online and reuses it from previous iterations. from roerich.algorithms import OnlineNNClassifier # detection cpd = OnlineNNClassifier(net='default', scaler=\"default\", metric=\"KL_sym\", periods=1, window_size=1, lag_size=100, step=1, n_epochs=10, lr=0.1, lam=0.0001, optimizer=\"Adam\") score, cps_pred = cpd.predict(X) # visualization roerich.display(X, cps_true, score, cps_pred) OnlineNNRuLSIF OnlineNNRuLSIF is the same as OnlineNNClassifier , but uses Neural Network regression model with the RuLSIF loss function fitted online. from roerich.algorithms import OnlineNNRuLSIF cpd = OnlineNNRuLSIF(alpha=0.05, net='default', scaler=\"default\", periods=1, window_size=1, lag_size=100, step=1, n_epochs=10, lr=0.1, lam=0.0001, optimizer=\"Adam\") score, cps_pred = cpd.predict(X) cps_pred array([ 198, 402, 601, 798, 1006, 1203, 1401, 1604, 1804])","title":"Basics"},{"location":"examples/demo/#basics","text":"","title":"Basics"},{"location":"examples/demo/#generate-signal","text":"In this example, we consider basic usages for change point detection methods in roerich library. Let\u2019s generate a signal with different regimes, or, in other words, with different distributions of observations. The goal is to determine all change points that correspond to the distribution changes. We also can consider this task as the signal segmentation, where each segment corresponds to one regime of the signal. import roerich import roerich.algorithms import roerich.metrics import numpy as np # generate a time series with change points X, cps_true = roerich.generate_dataset(period=200, N_tot=2000)","title":"Generate signal"},{"location":"examples/demo/#changepointdetectionclassifier","text":"The first method we use here is ChangePointDetectionClassifier . It is based on binary classifiers in machine learning. A classifier takes two parts of the signal with window_size observations and separates them into two classes. Then, it uses predictions to estimate the probability density ratio for these observations and calculate the change point score between the windows. The method refits the classifier on each new pair of windows. from roerich.algorithms import ChangePointDetectionClassifier from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis # sklearn-like binary classifier clf = QuadraticDiscriminantAnalysis() # change points detection cpd = ChangePointDetectionClassifier(base_classifier=clf, metric='KL_sym', periods=1, window_size=100, step=1, n_runs=1) score, cps_pred = cpd.predict(X) # visualization roerich.display(X, cps_true, score, cps_pred) Now, let\u2019s measure the quality of change point detection using Precision and Recall metrics. # metrics precision, recall = roerich.metrics.precision_recall_scores(cps_true, cps_pred, window=20) auc = roerich.metrics.pr_auc(cps_true, cps_pred, score[cps_pred], window=20) print('Precision: ', precision) print('Recall: ', recall) print('PR AUC: ', auc) Precision: 1.0 Recall: 1.0 PR AUC: 1.0 The ChangePointDetectionClassifier method can take any sklearn-like binary classifier. For example, you use your favorite Neural Network or Gradient Boosting over Decision Trees algorithm. The following example uses a shallow Neural Network implemented in PyTroch as a base classifier for change point detection. # pytorch NN classifier clf = roerich.algorithms.NNClassifier(n_hidden=10, n_epochs=10, batch_size=64, lr=0.1, l2=0.0) # detection cpd = ChangePointDetectionClassifier(base_classifier=clf, metric='KL_sym', periods=1, window_size=100, step=1, n_runs=1) score, cps_pred = cpd.predict(X) cps_pred array([ 194, 399, 598, 803, 1001, 1199, 1399, 1598, 1800])","title":"ChangePointDetectionClassifier"},{"location":"examples/demo/#changepointdetectionrulsif","text":"Similarly, ChangePointDetectionRuLSIF method also estimates probability density ratio between two windows of the signal. However, it does it using regression models with the RuLSIF loss function. They directly predict the ratios without learning individual distributions. Roerich provides two following RuLSIF models. NNRuLSIFRegressor model is a shallow Neural Network implemented in PyTorch. And GBDTRuLSIFRegressor is a Gradient Boosting over Regression Trees. The following examples show how to use them for change point detection. from roerich.algorithms import NNRuLSIFRegressor, ChangePointDetectionRuLSIF # pytorch NN regressor with RuLSIF loss function reg = NNRuLSIFRegressor(n_hidden=10, n_epochs=10, batch_size=64, lr=0.1, l2=0.0, alpha=0.05) # detection cpd = ChangePointDetectionRuLSIF(reg, metric='PE', periods=1, window_size=100, step=1, n_runs=1) score, cps_pred = cpd.predict(X) # visualization roerich.display(X, cps_true, score, cps_pred) from roerich.algorithms import GBDTRuLSIFRegressor # regressor with RuLSIF loss function reg = GBDTRuLSIFRegressor(n_estimators=10, max_depth=2) # detection cpd = ChangePointDetectionRuLSIF(reg, metric='PE', periods=1, window_size=100, step=1, n_runs=1) score, cps_pred = cpd.predict(X) cps_pred array([ 206, 400, 596, 798, 1001, 1201, 1401, 1600, 1799])","title":"ChangePointDetectionRuLSIF"},{"location":"examples/demo/#onlinennclassifier","text":"OnlineNNClassifier is based on the same principles as the previous methods. But it uses a single Neural Network to detect change points in the whole time series. The network scans the signal point by point ( window_size=1 ) and updates its weights online. It estimates the probability density ratio between distant ( lag_size=100 ) observations of the signal. In result, the method is faster than previous algorithms, because it fits the network online and reuses it from previous iterations. from roerich.algorithms import OnlineNNClassifier # detection cpd = OnlineNNClassifier(net='default', scaler=\"default\", metric=\"KL_sym\", periods=1, window_size=1, lag_size=100, step=1, n_epochs=10, lr=0.1, lam=0.0001, optimizer=\"Adam\") score, cps_pred = cpd.predict(X) # visualization roerich.display(X, cps_true, score, cps_pred)","title":"OnlineNNClassifier"},{"location":"examples/demo/#onlinennrulsif","text":"OnlineNNRuLSIF is the same as OnlineNNClassifier , but uses Neural Network regression model with the RuLSIF loss function fitted online. from roerich.algorithms import OnlineNNRuLSIF cpd = OnlineNNRuLSIF(alpha=0.05, net='default', scaler=\"default\", periods=1, window_size=1, lag_size=100, step=1, n_epochs=10, lr=0.1, lam=0.0001, optimizer=\"Adam\") score, cps_pred = cpd.predict(X) cps_pred array([ 198, 402, 601, 798, 1006, 1203, 1401, 1604, 1804])","title":"OnlineNNRuLSIF"},{"location":"examples/seismic/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Earthquake detection Description This example is about an earthquake detection using change point detection methods. We use data from Global Seismogram Viewer (GSV) developed by the Incorporated Research Institutions for Seismology (IRIS) . The viewer shows seismic wave arrivals for large earthquakes at seismic recording stations around the world. Scientists use the arrival times to identify the location of the earthquake, determine how deep it was, exactly when it occurred, and for other purposes. These seismic records also help to explore Earth\u2019s inner structure, because the traveling waves bounce and diffract at different speeds and angles, depending on the density, solidity and other characteristics of the Earth. In this example, we estimate seismic wave arrival using the change point detection technique. We consider an earthquake that occurred on March 11, 2011 near the east coast of Honshu in Japan, and has a magnitude of 9.1. A range of stations recorded the wave from this earthquake over the world. We took data from CASY station in Antarctica in 11825 km from the center of the earthquake. Data download We use IRIS URL Builder Web Service to download seismograms from the station. In this example, we use data in the BHZ channel, recorded with a sample rate of 20 Hz, and contains 216000 records for 3 hours. Run the following lines of code to download the data and unzip it. # !wget -O \"fdsnws.zip\" \"http://service.iris.edu/fdsnws/dataselect/1/query?net=IU&sta=CASY&starttime=2011-03-11T05:00:00&endtime=2011-03-11T08:00:00&format=geocsv.zip\" # !unzip -o -q fdsnws.zip -d fdsnws # import of basic libraries import matplotlib.pyplot as plt from matplotlib import cm import pandas as pd import numpy as np import roerich Now we read the data file. It contains metadata in the first 18 rows and two columns: Time: timestamps of measurements; Sample: seismometer measurements. path = \"fdsnws/IU.CASY.00.BHZ.M.2011-03-11T050000.024220.csv\" data = pd.read_csv(path, skiprows=18) data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Time Sample 0 2011-03-11T05:00:00.024220Z 2854 1 2011-03-11T05:00:00.074220Z 2804 2 2011-03-11T05:00:00.124220Z 2740 3 2011-03-11T05:00:00.174220Z 2668 4 2011-03-11T05:00:00.224220Z 2588 The file contains 216000 observations. We take only each 10th of them. It is enough for our exercise. Moreover, we manually found the seismic wave arrival time. The figure below shows a waveform of the signal. # take only earch 10th observation X = data[[' Sample']].values[::10] # wave arrival manually determined cps_true = [7350] # visualization roerich.display(X, cps_true, score=None, cps_pred=None) Preprocessing As always, we preprocess the data. We scale it with StandardScaler and add some noise to the signal. It is optional, but sometimes it can be helpful. Moreover, adding noise is a regularization technique for change point detection methods. It helps to reduce the number of false alarms, for example. from sklearn.preprocessing import StandardScaler # scale the signal ss = StandardScaler().fit(X) X_ss = ss.transform(X) # add noise for regularization X_ss += np.random.normal(0, 0.01, X_ss.shape) Change point detection We apply ChangePointDetectionClassifier method to identify the seismic wave arrival time. Here, we use RandomForestClassifier as a basic classifier. You can use any other binary classifier you like. The figures below show, that the change point score has several peaks. The highest one correspond to the arrival time. Others indicate changes in the seismic wave regimes. from roerich.algorithms import ChangePointDetectionClassifier from sklearn.ensemble import RandomForestClassifier # sklearn-like binary classifier clf = RandomForestClassifier(n_estimators=10, max_depth=4) # change points detection cpd = ChangePointDetectionClassifier(base_classifier=clf, metric='KL_sym', periods=1, window_size=1000, step=10, n_runs=1) score, cps_pred = cpd.predict(X_ss) # take a CP with the highest score cps_score = score[cps_pred] cps_pred_max = cps_pred[cps_score.argmax()] # visualization roerich.display(X_ss, cps_true, score, cps_pred_max) Spectrogram In the previous sections, we used the waveform. Now, let\u2019s apply Fast Fourier Transform (FFT) to our signal to obtain its spectrogram. This technique is widely used in sound analysis and other signal processing tasks. To do this, we use Spectrogram from torchaudio library. from torchaudio.transforms import Spectrogram import torch # define the spectrogram parameters featurizer = Spectrogram(n_fft=64, win_length=64, hop_length=1, power=2, normalized=True) def apply_compression(spec): return np.log(spec.clamp(1e-2)) # transform the waveform into spectrogram spec = featurizer(torch.Tensor(X.reshape(-1,)))[:, :-1] spec = apply_compression(spec) spec.shape torch.Size([33, 21600]) # display the spectrogram plt.figure(figsize=(12, 3)) plt.imshow(spec, aspect='auto', cmap=cm.bwr) plt.xlabel('Frame') plt.ylabel('Frequency bin') plt.show() Detection with spectrogram Finally, we repeat all previous steps to find change points in the spectrogram. # spec into numpy array XX = spec.T.numpy() # scale the signal ss = StandardScaler().fit(XX) XX_ss = ss.transform(XX) # add noise for regularization XX_ss += np.random.normal(0, 0.1, XX_ss.shape) # sklearn-like binary classifier clf = RandomForestClassifier(n_estimators=10, max_depth=4) # change points detection cpd = ChangePointDetectionClassifier(base_classifier=clf, metric='KL_sym', periods=1, window_size=1000, step=10, n_runs=1) score, cps_pred = cpd.predict(XX_ss) The figure shows that the results obtained based on the waveform and spectrogram are similar. The seismic wave arrival times are the same and have the highest scores. # take a CP with the highest score cps_score = score[cps_pred] cps_pred_max = cps_pred[cps_score.argmax()] # visualization roerich.display(X_ss, cps_true, score, cps_pred_max) # plot spectrogram plt.figure(figsize=(12, 6)) plt.subplot(2, 1, 1) plt.imshow(spec, aspect='auto', cmap=cm.bwr) xmin, xmax, ymin, ymax = plt.axis() plt.plot(cps_true*2, [ymin, ymax], color='0', linestyle='--') # plot change point score plt.subplot(2, 1, 2) plt.plot(score, linewidth=3, label=\"Change point score\", color='C3') plt.plot(cps_true*2, [score.min(), score.max()], color='0', linestyle='--') plt.xlim(xmin, xmax) plt.ylim([score.min(), score.max()]) plt.xlabel('Time') plt.legend() plt.show()","title":"Earthquake detection"},{"location":"examples/seismic/#earthquake-detection","text":"","title":"Earthquake detection"},{"location":"examples/seismic/#description","text":"This example is about an earthquake detection using change point detection methods. We use data from Global Seismogram Viewer (GSV) developed by the Incorporated Research Institutions for Seismology (IRIS) . The viewer shows seismic wave arrivals for large earthquakes at seismic recording stations around the world. Scientists use the arrival times to identify the location of the earthquake, determine how deep it was, exactly when it occurred, and for other purposes. These seismic records also help to explore Earth\u2019s inner structure, because the traveling waves bounce and diffract at different speeds and angles, depending on the density, solidity and other characteristics of the Earth. In this example, we estimate seismic wave arrival using the change point detection technique. We consider an earthquake that occurred on March 11, 2011 near the east coast of Honshu in Japan, and has a magnitude of 9.1. A range of stations recorded the wave from this earthquake over the world. We took data from CASY station in Antarctica in 11825 km from the center of the earthquake.","title":"Description"},{"location":"examples/seismic/#data-download","text":"We use IRIS URL Builder Web Service to download seismograms from the station. In this example, we use data in the BHZ channel, recorded with a sample rate of 20 Hz, and contains 216000 records for 3 hours. Run the following lines of code to download the data and unzip it. # !wget -O \"fdsnws.zip\" \"http://service.iris.edu/fdsnws/dataselect/1/query?net=IU&sta=CASY&starttime=2011-03-11T05:00:00&endtime=2011-03-11T08:00:00&format=geocsv.zip\" # !unzip -o -q fdsnws.zip -d fdsnws # import of basic libraries import matplotlib.pyplot as plt from matplotlib import cm import pandas as pd import numpy as np import roerich Now we read the data file. It contains metadata in the first 18 rows and two columns: Time: timestamps of measurements; Sample: seismometer measurements. path = \"fdsnws/IU.CASY.00.BHZ.M.2011-03-11T050000.024220.csv\" data = pd.read_csv(path, skiprows=18) data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Time Sample 0 2011-03-11T05:00:00.024220Z 2854 1 2011-03-11T05:00:00.074220Z 2804 2 2011-03-11T05:00:00.124220Z 2740 3 2011-03-11T05:00:00.174220Z 2668 4 2011-03-11T05:00:00.224220Z 2588 The file contains 216000 observations. We take only each 10th of them. It is enough for our exercise. Moreover, we manually found the seismic wave arrival time. The figure below shows a waveform of the signal. # take only earch 10th observation X = data[[' Sample']].values[::10] # wave arrival manually determined cps_true = [7350] # visualization roerich.display(X, cps_true, score=None, cps_pred=None)","title":"Data download"},{"location":"examples/seismic/#preprocessing","text":"As always, we preprocess the data. We scale it with StandardScaler and add some noise to the signal. It is optional, but sometimes it can be helpful. Moreover, adding noise is a regularization technique for change point detection methods. It helps to reduce the number of false alarms, for example. from sklearn.preprocessing import StandardScaler # scale the signal ss = StandardScaler().fit(X) X_ss = ss.transform(X) # add noise for regularization X_ss += np.random.normal(0, 0.01, X_ss.shape)","title":"Preprocessing"},{"location":"examples/seismic/#change-point-detection","text":"We apply ChangePointDetectionClassifier method to identify the seismic wave arrival time. Here, we use RandomForestClassifier as a basic classifier. You can use any other binary classifier you like. The figures below show, that the change point score has several peaks. The highest one correspond to the arrival time. Others indicate changes in the seismic wave regimes. from roerich.algorithms import ChangePointDetectionClassifier from sklearn.ensemble import RandomForestClassifier # sklearn-like binary classifier clf = RandomForestClassifier(n_estimators=10, max_depth=4) # change points detection cpd = ChangePointDetectionClassifier(base_classifier=clf, metric='KL_sym', periods=1, window_size=1000, step=10, n_runs=1) score, cps_pred = cpd.predict(X_ss) # take a CP with the highest score cps_score = score[cps_pred] cps_pred_max = cps_pred[cps_score.argmax()] # visualization roerich.display(X_ss, cps_true, score, cps_pred_max)","title":"Change point detection"},{"location":"examples/seismic/#spectrogram","text":"In the previous sections, we used the waveform. Now, let\u2019s apply Fast Fourier Transform (FFT) to our signal to obtain its spectrogram. This technique is widely used in sound analysis and other signal processing tasks. To do this, we use Spectrogram from torchaudio library. from torchaudio.transforms import Spectrogram import torch # define the spectrogram parameters featurizer = Spectrogram(n_fft=64, win_length=64, hop_length=1, power=2, normalized=True) def apply_compression(spec): return np.log(spec.clamp(1e-2)) # transform the waveform into spectrogram spec = featurizer(torch.Tensor(X.reshape(-1,)))[:, :-1] spec = apply_compression(spec) spec.shape torch.Size([33, 21600]) # display the spectrogram plt.figure(figsize=(12, 3)) plt.imshow(spec, aspect='auto', cmap=cm.bwr) plt.xlabel('Frame') plt.ylabel('Frequency bin') plt.show()","title":"Spectrogram"},{"location":"examples/seismic/#detection-with-spectrogram","text":"Finally, we repeat all previous steps to find change points in the spectrogram. # spec into numpy array XX = spec.T.numpy() # scale the signal ss = StandardScaler().fit(XX) XX_ss = ss.transform(XX) # add noise for regularization XX_ss += np.random.normal(0, 0.1, XX_ss.shape) # sklearn-like binary classifier clf = RandomForestClassifier(n_estimators=10, max_depth=4) # change points detection cpd = ChangePointDetectionClassifier(base_classifier=clf, metric='KL_sym', periods=1, window_size=1000, step=10, n_runs=1) score, cps_pred = cpd.predict(XX_ss) The figure shows that the results obtained based on the waveform and spectrogram are similar. The seismic wave arrival times are the same and have the highest scores. # take a CP with the highest score cps_score = score[cps_pred] cps_pred_max = cps_pred[cps_score.argmax()] # visualization roerich.display(X_ss, cps_true, score, cps_pred_max) # plot spectrogram plt.figure(figsize=(12, 6)) plt.subplot(2, 1, 1) plt.imshow(spec, aspect='auto', cmap=cm.bwr) xmin, xmax, ymin, ymax = plt.axis() plt.plot(cps_true*2, [ymin, ymax], color='0', linestyle='--') # plot change point score plt.subplot(2, 1, 2) plt.plot(score, linewidth=3, label=\"Change point score\", color='C3') plt.plot(cps_true*2, [score.min(), score.max()], color='0', linestyle='--') plt.xlim(xmin, xmax) plt.ylim([score.min(), score.max()]) plt.xlabel('Time') plt.legend() plt.show()","title":"Detection with spectrogram"},{"location":"examples/wisdm/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Human activity recognition Description This example is about change points detection for a human activity recognition task. We use \u201cWISDM Smartphone and Smartwatch Activity and Biometrics Dataset\u201d [1, 2], prepared by the Wireless Sensor Data Mining (WISDM) Lab in the Department of Computer and Information Science of Fordham University. The dataset includes data from the accelerometer and gyroscope sensors of a smartphone and smartwatch collected as 51 subjects performed 18 diverse activities of daily living. Each activity was performed for 3 minutes, so that each subject contributed 54 minutes of data. These activities include basic ambulation-related activities (e.g., walking, jogging, climbing stairs), hand-based activities of daily living (e.g., brushing teeth, folding clothes), and various eating activities (eating pasta, easting chips). Each subject had a smartwatch placed on his/her dominant hand and a smartphone in their pocket. The data collection was controlled by a custom-made app that ran on the smartphone and smartwatch. The sensor data was collected at a rate of 20 Hz (i.e., every 50ms) from the accelerometer and gyroscope on both the smartphone and smartwatch, yielding four total sensors. [1] Weiss, Gary. (2019). WISDM Smartphone and Smartwatch Activity and Biometrics Dataset . UCI Machine Learning Repository. [link] [2] Weiss, Gary & Yoneda, Kenichi & Hayajneh, Thaier. (2019). Smartphone and Smartwatch-Based Biometrics Using Activities of Daily Living. IEEE Access. PP. 1-1. [DOI] . Data download The dataset is stored in UCI Machine Learning Repository and available for download via this link . #!wget -q https://archive.ics.uci.edu/ml/machine-learning-databases/00507/wisdm-dataset.zip #!unzip -o -q wisdm-dataset.zip In this example, we use data from a smartwatch accelerometer collected from one subject. The data file contains the following columns: id: uniquely identifies the subject. Rand: 1600 - 1650; activity: identifies a specific activity. Range: A-S (no \u201cN\u201d value); timestamp: Linux time; x: sensor value for x axis. May be positive or negative; y: same as x but for y axis; z: same as x but for z axis; Let\u2019s download the dataset and read a sample. # import of basic libraries import matplotlib.pyplot as plt import pandas as pd import numpy as np import roerich # helping function for reading the data def remove_char(fname): with open(fname, \"r\") as f: flist = f.readlines() flist = [s.replace(';', '') for s in flist] with open(fname, \"w\") as f: f.writelines(flist) # path to a file in the dataset data_path = \"wisdm-dataset/raw/watch/accel/data_1600_accel_watch.txt\" # remove ';' from the file remove_char(data_path) # read the data cols = [\"id\", \"activity\", \"timestamp\", \"x\", \"y\", \"z\"] data = pd.read_csv(data_path, names=cols) data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id activity timestamp x y z 0 1600 A 90426708196641 7.091625 -0.591667 8.195502 1 1600 A 90426757696641 4.972757 -0.158317 6.696732 2 1600 A 90426807196641 3.253720 -0.191835 6.107758 3 1600 A 90426856696641 2.801216 -0.155922 5.997625 4 1600 A 90426906196641 3.770868 -1.051354 7.731027 Preprocessing We will use only sensor measurements and activity labels. So, let\u2019s define the input matrix X and the label vector y . X = data[['x', 'y', 'z']].values y = data['activity'].values However, we do not need the activity labels by itself, but moments of the activity changes. We consider them as change points of the signal, and our goal is to detect them. # transform activity labels into change point positions def get_true_cpds(y): cps_true = [] for i in range(1, len(y)): if y[i] != y[i-1]: cps_true.append(i) return np.array(cps_true) # get true change point positions cps_true = get_true_cpds(y) # visualization roerich.display(X, cps_true, score=None, cps_pred=None) Then, we preprocess the time series using StandardScaler . It is not mandatory in general case, but sometimes can be useful :). Besides this, we add some noise to our signal. This hack is a regularization technique that helps to reduce sensitivity of the change point detection methods. It decreases the number of change points within one activity. from sklearn.preprocessing import StandardScaler # scale the signal ss = StandardScaler().fit(X) X_ss = ss.transform(X) # add noise for regularization X_ss += np.random.normal(0, 0.1, X_ss.shape) Change point detection We use ChangePointDetectionClassifier method to estimate switches between different regimes of the time series. Then, the found change points can be used for the sensor signal segmentation into different human activities. from roerich.algorithms import ChangePointDetectionClassifier from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis # sklearn-like binary classifier clf = QuadraticDiscriminantAnalysis() # change points detection cpd = ChangePointDetectionClassifier(base_classifier=clf, metric='KL_sym', periods=1, window_size=1000, step=10, n_runs=1) score, cps_pred = cpd.predict(X_ss) # visualization roerich.display(X_ss, cps_true, score, cps_pred) Quality metrics Finally, we calculate Precision and Recall metrics to measure quality of the detected change points. In addition, we plot Precision-Recall curve using different thresholds for the detection score. from roerich.metrics import precision_recall_scores, precision_recall_curve, auc_score # precision and recall precision, recall = precision_recall_scores(cps_true, cps_pred, window=50) print('Precision: ', precision) print('Recall: ', recall) # PR curve and AUC thr, precision, recall = precision_recall_curve(cps_true, cps_pred, score[cps_pred], window=50) auc = auc_score(thr, precision, recall) print(\"PR AUC: \", auc) # visualization plt.plot(recall, precision) plt.scatter(recall, precision) plt.xlabel(\"Recall\", size=14) plt.ylabel(\"Precision\", size=14) plt.grid() plt.show() Precision: 0.8947368421052632 Recall: 1.0 PR AUC: 0.9966359092656671","title":"Human activity recognition"},{"location":"examples/wisdm/#human-activity-recognition","text":"","title":"Human activity recognition"},{"location":"examples/wisdm/#description","text":"This example is about change points detection for a human activity recognition task. We use \u201cWISDM Smartphone and Smartwatch Activity and Biometrics Dataset\u201d [1, 2], prepared by the Wireless Sensor Data Mining (WISDM) Lab in the Department of Computer and Information Science of Fordham University. The dataset includes data from the accelerometer and gyroscope sensors of a smartphone and smartwatch collected as 51 subjects performed 18 diverse activities of daily living. Each activity was performed for 3 minutes, so that each subject contributed 54 minutes of data. These activities include basic ambulation-related activities (e.g., walking, jogging, climbing stairs), hand-based activities of daily living (e.g., brushing teeth, folding clothes), and various eating activities (eating pasta, easting chips). Each subject had a smartwatch placed on his/her dominant hand and a smartphone in their pocket. The data collection was controlled by a custom-made app that ran on the smartphone and smartwatch. The sensor data was collected at a rate of 20 Hz (i.e., every 50ms) from the accelerometer and gyroscope on both the smartphone and smartwatch, yielding four total sensors. [1] Weiss, Gary. (2019). WISDM Smartphone and Smartwatch Activity and Biometrics Dataset . UCI Machine Learning Repository. [link] [2] Weiss, Gary & Yoneda, Kenichi & Hayajneh, Thaier. (2019). Smartphone and Smartwatch-Based Biometrics Using Activities of Daily Living. IEEE Access. PP. 1-1. [DOI] .","title":"Description"},{"location":"examples/wisdm/#data-download","text":"The dataset is stored in UCI Machine Learning Repository and available for download via this link . #!wget -q https://archive.ics.uci.edu/ml/machine-learning-databases/00507/wisdm-dataset.zip #!unzip -o -q wisdm-dataset.zip In this example, we use data from a smartwatch accelerometer collected from one subject. The data file contains the following columns: id: uniquely identifies the subject. Rand: 1600 - 1650; activity: identifies a specific activity. Range: A-S (no \u201cN\u201d value); timestamp: Linux time; x: sensor value for x axis. May be positive or negative; y: same as x but for y axis; z: same as x but for z axis; Let\u2019s download the dataset and read a sample. # import of basic libraries import matplotlib.pyplot as plt import pandas as pd import numpy as np import roerich # helping function for reading the data def remove_char(fname): with open(fname, \"r\") as f: flist = f.readlines() flist = [s.replace(';', '') for s in flist] with open(fname, \"w\") as f: f.writelines(flist) # path to a file in the dataset data_path = \"wisdm-dataset/raw/watch/accel/data_1600_accel_watch.txt\" # remove ';' from the file remove_char(data_path) # read the data cols = [\"id\", \"activity\", \"timestamp\", \"x\", \"y\", \"z\"] data = pd.read_csv(data_path, names=cols) data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id activity timestamp x y z 0 1600 A 90426708196641 7.091625 -0.591667 8.195502 1 1600 A 90426757696641 4.972757 -0.158317 6.696732 2 1600 A 90426807196641 3.253720 -0.191835 6.107758 3 1600 A 90426856696641 2.801216 -0.155922 5.997625 4 1600 A 90426906196641 3.770868 -1.051354 7.731027","title":"Data download"},{"location":"examples/wisdm/#preprocessing","text":"We will use only sensor measurements and activity labels. So, let\u2019s define the input matrix X and the label vector y . X = data[['x', 'y', 'z']].values y = data['activity'].values However, we do not need the activity labels by itself, but moments of the activity changes. We consider them as change points of the signal, and our goal is to detect them. # transform activity labels into change point positions def get_true_cpds(y): cps_true = [] for i in range(1, len(y)): if y[i] != y[i-1]: cps_true.append(i) return np.array(cps_true) # get true change point positions cps_true = get_true_cpds(y) # visualization roerich.display(X, cps_true, score=None, cps_pred=None) Then, we preprocess the time series using StandardScaler . It is not mandatory in general case, but sometimes can be useful :). Besides this, we add some noise to our signal. This hack is a regularization technique that helps to reduce sensitivity of the change point detection methods. It decreases the number of change points within one activity. from sklearn.preprocessing import StandardScaler # scale the signal ss = StandardScaler().fit(X) X_ss = ss.transform(X) # add noise for regularization X_ss += np.random.normal(0, 0.1, X_ss.shape)","title":"Preprocessing"},{"location":"examples/wisdm/#change-point-detection","text":"We use ChangePointDetectionClassifier method to estimate switches between different regimes of the time series. Then, the found change points can be used for the sensor signal segmentation into different human activities. from roerich.algorithms import ChangePointDetectionClassifier from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis # sklearn-like binary classifier clf = QuadraticDiscriminantAnalysis() # change points detection cpd = ChangePointDetectionClassifier(base_classifier=clf, metric='KL_sym', periods=1, window_size=1000, step=10, n_runs=1) score, cps_pred = cpd.predict(X_ss) # visualization roerich.display(X_ss, cps_true, score, cps_pred)","title":"Change point detection"},{"location":"examples/wisdm/#quality-metrics","text":"Finally, we calculate Precision and Recall metrics to measure quality of the detected change points. In addition, we plot Precision-Recall curve using different thresholds for the detection score. from roerich.metrics import precision_recall_scores, precision_recall_curve, auc_score # precision and recall precision, recall = precision_recall_scores(cps_true, cps_pred, window=50) print('Precision: ', precision) print('Recall: ', recall) # PR curve and AUC thr, precision, recall = precision_recall_curve(cps_true, cps_pred, score[cps_pred], window=50) auc = auc_score(thr, precision, recall) print(\"PR AUC: \", auc) # visualization plt.plot(recall, precision) plt.scatter(recall, precision) plt.xlabel(\"Recall\", size=14) plt.ylabel(\"Precision\", size=14) plt.grid() plt.show() Precision: 0.8947368421052632 Recall: 1.0 PR AUC: 0.9966359092656671","title":"Quality metrics"}]}